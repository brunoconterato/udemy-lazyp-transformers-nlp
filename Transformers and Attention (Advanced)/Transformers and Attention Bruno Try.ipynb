{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Let's build the transformer's encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0. Init\n",
    "\n",
    "* Initialize the notebook environment.\n",
    "* Hyperparameters definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters Here\n",
    "BATCH_SIZE = 128\n",
    "T = 80  # sentence length\n",
    "D_K = 16\n",
    "D_V = 16\n",
    "D_MODEL = 128\n",
    "H = 8\n",
    "VOCAB_SIZE = 10\n",
    "N_TRANSFORMERS_BLOCKS_ENCODER = 6\n",
    "N_CLASSES = 2  # classes of classifier\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "RUN_UNIT_TESTS = True\n",
    "TRAIN_MODEL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "# Test softmax x axis:\n",
    "matrix = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "softmaxed_matrix = F.softmax(matrix, dim=1)\n",
    "\n",
    "print(softmaxed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(x: torch.Tensor, n: int):\n",
    "    # make shape (n, 1, 1, ...) --> quantity of 1's must be len(x.shape)\n",
    "    # for example, if shape of x is (3, 4, 8), shapee must be (n, 1, 1, 1)\n",
    "    tuple_ones = tuple(\n",
    "        (torch.tensor(x.shape) / torch.tensor(x.shape)).numpy().astype(int)\n",
    "    )\n",
    "    return x.unsqueeze(0).repeat((n, *tuple_ones))\n",
    "\n",
    "\n",
    "def batched_matmul(tensor_3d, tensor_2d):\n",
    "    W_repeated = repeat(tensor_2d, n=tensor_3d.shape[0])\n",
    "\n",
    "    return torch.bmm(tensor_3d, W_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Convention from: https://www.udemy.com/course/data-science-transformers-nlp/learn/lecture/32255056#overview\n",
    "    In our convention, K, Q and V are learneable, different from the \"Attention is all you need\" paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_K, d_V, d_model: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Define trainable parameters with requires_grad=True\n",
    "        # torch 2d tensor initialized normally\n",
    "        self.W_K = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_K)))\n",
    "        self.W_Q = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_K)))\n",
    "        self.W_V = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_V)))\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the Attention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch, T, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch, T, d_V)\n",
    "            torch.Tensor: Attention weights of shape (batch, T, T)\n",
    "        \"\"\"\n",
    "        # Shapes:\n",
    "        # x is a 3d tensor (batch, T, d_model)\n",
    "        # W_K (d_model, d_K)\n",
    "\n",
    "        # (batch, T, d_model) x (d_model, d_K) -> (batch, T, d_K)\n",
    "        K = batched_matmul(x, self.W_K)\n",
    "        Q = batched_matmul(x, self.W_Q)\n",
    "        V = batched_matmul(x, self.W_V)\n",
    "\n",
    "        # (batch, T, d_K) x (batch, d_k, T) -> (batch, T, T)\n",
    "        result = torch.bmm(Q, K.transpose(1, 2)) / (K.shape[-1] ** 0.5)\n",
    "        if self.mask:\n",
    "            result = torch.bmm(result, self.mask)\n",
    "        attention_weights = F.softmax(result, dim=-1)\n",
    "        # (batch, T, T) x (batch, T, d_V) -> (batch, T, d_V)\n",
    "        result = torch.bmm(attention_weights, V)\n",
    "        return result, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16])\n",
      "torch.Size([128, 16])\n",
      "torch.Size([128, 16])\n",
      "torch.Size([128, 80, 16])\n"
     ]
    }
   ],
   "source": [
    "att = Attention(d_K=D_K, d_V=D_V, d_model=D_MODEL)\n",
    "x = torch.normal(mean=0, std=0.01, size=(BATCH_SIZE, T, D_MODEL))\n",
    "\n",
    "att_result, att_weights = att.forward(x)\n",
    "assert att_result.shape == (BATCH_SIZE, T, D_V)\n",
    "print(att.W_K.shape)\n",
    "print(att.W_Q.shape)\n",
    "print(att.W_V.shape)\n",
    "print(att_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, d_K, d_V):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [Attention(d_K=d_K, d_V=d_V, d_model=d_model) for _ in range(h)]\n",
    "        )\n",
    "        self.W_O = nn.Parameter(torch.normal(mean=0, std=0.01, size=(h * d_V, d_model)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"Forward pass of the MultiHeadAttention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch, T, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch, T, d_model)\n",
    "            torch.Tensor: Attention weights of shape (batch, h, T, T)\n",
    "        \"\"\"\n",
    "        attention_results = [attention(x)[0] for attention in self.attentions]\n",
    "        attention_weights = torch.stack(\n",
    "            [attention(x)[1] for attention in self.attentions]\n",
    "        )\n",
    "        concatenated = torch.cat(attention_results, dim=-1)\n",
    "        return batched_matmul(concatenated, self.W_O), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(h=H, d_K=D_K, d_V=D_V, d_model=D_MODEL)\n",
    "out, att_weights = mha(x)\n",
    "assert out.shape == (BATCH_SIZE, T, D_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. The transformer block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (1): Dropout(p=0.1, inplace=False)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (4): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_K=D_K, d_V=D_V, d_model=D_MODEL, h=H, dropout=0.1, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mha = MultiHeadAttention(h, d_K=d_K, d_V=d_V, d_model=d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.layer_norm(x + self.mha(x)[0])\n",
    "        x = self.layer_norm(x + self.ann(x)[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "transformerBlock = TransformerBlock()\n",
    "transformerBlock(x).shape\n",
    "transformerBlock.ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. The positional encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PositionalEncoding(t: int, d_model) -> torch.Tensor:\n",
    "    encodings = torch.zeros(size=(t, d_model), requires_grad=False)\n",
    "    counter = 0\n",
    "    for pos in range(t):\n",
    "        for i in range((d_model // 2) + 1):\n",
    "            if 2 * i < d_model:\n",
    "                counter += 1\n",
    "                encodings[pos, 2 * i] = torch.sin(\n",
    "                    pos / torch.tensor(10000).pow(2 * i / d_model)\n",
    "                )\n",
    "            if 2 * i + 1 < d_model:\n",
    "                counter += 1\n",
    "                encodings[pos, 2 * i + 1] = torch.cos(\n",
    "                    pos / torch.tensor(10000).pow(2 * i / d_model)\n",
    "                )\n",
    "    assert counter == t * d_model\n",
    "    return encodings\n",
    "\n",
    "\n",
    "PositionalEncoding(T, D_MODEL).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35878/2721896283.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  torch.range(0, 10).reshape(-1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.range(0, 10).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. The embedding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding tensor([[-0.0835, -0.1685],\n",
      "        [ 0.0245,  0.0394],\n",
      "        [-0.0791, -0.0833]])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2, 2])\n",
      "tensor([[[-0.0791, -0.0833],\n",
      "         [ 0.0245,  0.0394]],\n",
      "\n",
      "        [[ 0.0245,  0.0394],\n",
      "         [-0.0835, -0.1685]],\n",
      "\n",
      "        [[-0.0835, -0.1685],\n",
      "         [-0.0791, -0.0833]]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: make this work in batches\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, padding_idx=0):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Parameter(\n",
    "            torch.normal(\n",
    "                mean=0.0, std=0.1, size=(vocab_size, d_model), requires_grad=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass Embedding layer\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input Tensor of shape (batch_size, T)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output Tensor of shape (batch_size, T, d_model)\n",
    "        \"\"\"\n",
    "        return self.embedding[x.long()]\n",
    "\n",
    "\n",
    "emb = Embedding(vocab_size=3, d_model=2)\n",
    "# emb = nn.Embedding(num_embeddings=3, embedding_dim=2)\n",
    "for name, param in emb.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "input_emb = torch.FloatTensor([[2, 1], [1, 0], [0, 2]])\n",
    "\n",
    "print(input_emb.shape)\n",
    "\n",
    "out_emb = emb(input_emb.long())\n",
    "print(out_emb.shape)\n",
    "print(out_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. The Classification Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make this class Encoder.\n",
    "# Then, make the ClassifierEncoder class be this class + a classifier head\n",
    "class ClassifierEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        t=T,\n",
    "        d_K=D_K,\n",
    "        d_V=D_V,\n",
    "        d_model=D_MODEL,\n",
    "        h=H,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        n_transformers=N_TRANSFORMERS_BLOCKS_ENCODER,\n",
    "        n_classes=N_CLASSES,\n",
    "        dropout=0.1,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ClassifierEncoder, self).__init__()\n",
    "        self.d_K = d_K\n",
    "        self.d_V = d_V\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.position_encoding: torch.Tensor = PositionalEncoding(t, d_model).to(device)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "        for _ in range(n_transformers):\n",
    "            self.transformer_blocks.append(\n",
    "                TransformerBlock(d_K=d_K, d_V=d_V, d_model=d_model, h=h)\n",
    "            )\n",
    "\n",
    "        self.prediction_head = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batchedPositionalEncoding = self.position_encoding.repeat(x.shape[0], 1, 1)\n",
    "        x = self.embedding(x.int()).float()\n",
    "        x = x + batchedPositionalEncoding\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.prediction_head(x)\n",
    "\n",
    "        x = F.softmax(x, dim=-1)\n",
    "\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2,  ..., 7, 8, 9],\n",
       "        [9, 0, 1,  ..., 6, 7, 8],\n",
       "        [8, 9, 0,  ..., 5, 6, 7],\n",
       "        ...,\n",
       "        [5, 6, 7,  ..., 2, 3, 4],\n",
       "        [4, 5, 6,  ..., 1, 2, 3],\n",
       "        [3, 4, 5,  ..., 0, 1, 2]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTestX(batch_size=BATCH_SIZE, t=T, vocab_size=VOCAB_SIZE):\n",
    "    x = torch.arange(start=0, end=batch_size * t).reshape(batch_size, t) % vocab_size\n",
    "\n",
    "    # each row (wor of index i) of x must be translocated by i positions\n",
    "    for i in range(batch_size):\n",
    "        x[i] = torch.roll(x[i], i)\n",
    "    return x\n",
    "\n",
    "\n",
    "getTestX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------\n",
      "x.shape: torch.Size([3, 4])\n",
      "x: tensor([[0, 1, 2, 3],\n",
      "        [1, 4, 5, 0],\n",
      "        [4, 5, 2, 3]])\n",
      "\n",
      "----------------------\n",
      "out.shape:  torch.Size([3, 2])\n",
      "out tensor([[0.5119, 0.4881],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5119, 0.4881]])\n",
      "\n",
      "----------------------\n",
      "expected_output.shape:  torch.Size([3, 2])\n",
      "expected_output tensor([[0.5119, 0.4881],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5119, 0.4881]])\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    matmul_qk = torch.matmul(Q, K.transpose(-2, -1))  # Scaled QK^T\n",
    "    d_k = Q.size(-1)\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(d_k)\n",
    "    attention_weights = F.softmax(\n",
    "        scaled_attention_logits, dim=-1\n",
    "    )  # Softmax over last axis (seq_len_k)\n",
    "    output = torch.matmul(attention_weights, V)  # Softmax(QK^T)V\n",
    "    return output\n",
    "\n",
    "\n",
    "def calculate_expected_output(x, model):\n",
    "    \"\"\"This function must calculate the expected output of the model, given the input x.\n",
    "    This function must not forward pass the model, but rather calculate the expected output\n",
    "    The expected output is function of the weights of the model, which are defined in the test_forward method\n",
    "    of the ClassifierEncoderTest class.\n",
    "    This function must follow the \"Attention is all you need\" paper.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of shape (batch, T)\n",
    "        model (_type_): _description_\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    t = x.shape[1]\n",
    "\n",
    "    embedded_x = model.embedding(x)\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(embedded_x, model.embedding(x))\n",
    "\n",
    "    def positional_encoding(max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        div_term = div_term.unsqueeze(0)\n",
    "\n",
    "        for i in range(max_len):\n",
    "            for j in range(0, d_model, 2):\n",
    "                pe[i, j] = torch.sin(position[i] * div_term[0, j // 2])\n",
    "                if j + 1 < d_model:\n",
    "                    pe[i, j + 1] = torch.cos(position[i] * div_term[0, j // 2])\n",
    "\n",
    "        return pe.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    position_encoding = positional_encoding(t, model.d_model)\n",
    "    assert position_encoding.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(position_encoding, model.position_encoding)\n",
    "\n",
    "    transformer_input = embedded_x + position_encoding\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(\n",
    "        transformer_input, model.embedding(x) + model.position_encoding\n",
    "    )\n",
    "\n",
    "    # Transformer Blocks\n",
    "    for block in model.transformer_blocks:\n",
    "        # Multi-Head Attention\n",
    "        mha_output = torch.zeros_like(transformer_input)\n",
    "        assert mha_output.shape == (batch_size, t, model.d_model)\n",
    "        head_outputs = torch.zeros((batch_size, t, model.d_V * model.h))\n",
    "        for head_idx, head in enumerate(block.mha.attentions):\n",
    "            Q = batched_matmul(transformer_input, head.W_Q)\n",
    "            K = batched_matmul(transformer_input, head.W_K)\n",
    "            V = batched_matmul(transformer_input, head.W_V)\n",
    "            head_output = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "            # health checking forward: assert model forward equals simulated forward\n",
    "            assert torch.allclose(head_output, head.forward(transformer_input)[0])\n",
    "\n",
    "            # Assert the shape of the output of the head,\n",
    "            # bases on the \"Attention is all you need\" paper\n",
    "            assert head_output.shape == (batch_size, t, model.d_V)\n",
    "\n",
    "            head_outputs[\n",
    "                :, :, model.d_V * head_idx : model.d_V * (head_idx + 1)\n",
    "            ] = head_output\n",
    "\n",
    "        mha_output += batched_matmul(head_outputs, block.mha.W_O)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(mha_output, block.mha(transformer_input)[0])\n",
    "\n",
    "        # Layer Normalization\n",
    "        normed_mha_output = block.layer_norm(\n",
    "            mha_output + transformer_input\n",
    "        )  # Assuming residual connection\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(\n",
    "            normed_mha_output,\n",
    "            block.layer_norm(block.mha(transformer_input)[0] + transformer_input),\n",
    "        )\n",
    "\n",
    "        # Do it now...\n",
    "        # 1. Linear\n",
    "        ann_output = (\n",
    "            torch.matmul(normed_mha_output, block.ann[0].weight.T) + block.ann[0].bias\n",
    "        )\n",
    "        assert ann_output.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(ann_output, block.ann[0](normed_mha_output))\n",
    "\n",
    "        # 2. Dropout\n",
    "        # ann_output = block.ann[1](ann_output)\n",
    "        # assert ann_output.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # 3. ReLU\n",
    "        ann_output_relu = F.relu(ann_output)\n",
    "        assert ann_output_relu.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(ann_output_relu, block.ann[2](ann_output))\n",
    "\n",
    "        # 4. Linear\n",
    "        ann_output_linear_2 = (\n",
    "            torch.matmul(ann_output_relu, block.ann[3].weight.T) + block.ann[3].bias\n",
    "        )\n",
    "        assert ann_output_linear_2.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(ann_output_linear_2, block.ann[3](ann_output_relu))\n",
    "\n",
    "        # 5. Dropout\n",
    "        # ann_output_linear_2 = block.ann[4](ann_output_linear_2)\n",
    "        # assert ann_output_linear_2.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Layer Normalization\n",
    "        normed_ann_output_linear_2 = block.layer_norm(\n",
    "            ann_output_linear_2 + normed_mha_output\n",
    "        )\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(\n",
    "            normed_ann_output_linear_2,\n",
    "            block.layer_norm(ann_output_linear_2 + normed_mha_output),\n",
    "        )\n",
    "\n",
    "        # Prepare for next block\n",
    "        transformer_input = normed_ann_output_linear_2\n",
    "        # print(\"\\n----------------------\")\n",
    "        # print(\"transformer output: \", transformer_input)\n",
    "\n",
    "    # Prediction Head\n",
    "\n",
    "    # 1. Linear\n",
    "    output_linear = (\n",
    "        torch.matmul(transformer_input, model.prediction_head.weight.T)\n",
    "        + model.prediction_head.bias\n",
    "    )\n",
    "    assert output_linear.shape == (batch_size, t, model.n_classes)\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(output_linear, model.prediction_head(transformer_input))\n",
    "\n",
    "    # 2. Dropout\n",
    "    # final_output = model.prediction_head[1](final_output)\n",
    "    # assert final_output.shape == (batch_size, t, model.n_classes)\n",
    "\n",
    "    # 3. Softmax\n",
    "    final_output = nn.Softmax(dim=-1)(output_linear)\n",
    "\n",
    "    # return only the last value of the sequence\n",
    "    final_output = final_output[:, -1, :]\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(\n",
    "        final_output,\n",
    "        F.softmax(model.prediction_head(transformer_input), dim=-1)[:, -1, :],\n",
    "    )\n",
    "\n",
    "    return final_output\n",
    "\n",
    "\n",
    "# Lets make unit test for ClassiofierEncoder\n",
    "class ClassifierEncoderTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 3\n",
    "        self.d_k = 3\n",
    "        self.d_v = 3\n",
    "        self.t = 4\n",
    "        self.d_model = 3\n",
    "        self.h = 2\n",
    "        self.t_blocks = 2\n",
    "        self.vocab_size = 6\n",
    "\n",
    "        self.model = ClassifierEncoder(\n",
    "            t=self.t,\n",
    "            d_K=self.d_k,\n",
    "            d_V=self.d_v,\n",
    "            d_model=self.d_model,\n",
    "            h=self.h,\n",
    "            vocab_size=self.vocab_size,\n",
    "            n_transformers=self.t_blocks,\n",
    "            n_classes=N_CLASSES,\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # print parameters of the model\n",
    "        # print(\"\\n----------------------\")\n",
    "        # for name, param in self.model.named_parameters():\n",
    "        #     print(name, param.data)\n",
    "\n",
    "    # unittest the forward pass\n",
    "    def test_forward(self):\n",
    "        # Create a random input\n",
    "        x = getTestX(self.batch_size, self.t, self.vocab_size)\n",
    "        print(\"\\n----------------------\")\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        print(f\"x: {x}\")\n",
    "\n",
    "        # define the weights of the model\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 1. define weights for the embedding layer\n",
    "            self.model.embedding.embedding = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        [0.1, 0.05, 0.02],\n",
    "                        [0.03, 0.07, 0.01],\n",
    "                        [0.04, 0.02, 0.08],\n",
    "                        [0.06, 0.03, 0.09],\n",
    "                        [0.02, 0.06, 0.04],\n",
    "                        [0.05, 0.01, 0.07],\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            # assert embedding has shape (vocab_size, d_model)\n",
    "            self.assertEqual(\n",
    "                self.model.embedding.embedding.shape,\n",
    "                torch.Size((self.vocab_size, self.d_model)),\n",
    "            )\n",
    "\n",
    "            # 2. define weights for the transformer blocks\n",
    "\n",
    "            # 2.1. define weights for the multihead attention layers\n",
    "            for t_idx in range(self.t_blocks):\n",
    "                for a_idx in range(self.h):\n",
    "                    self.model.transformer_blocks[t_idx].mha.attentions[\n",
    "                        a_idx\n",
    "                    ].W_Q = nn.Parameter(\n",
    "                        torch.tensor(\n",
    "                            [[0.1, 0.05, 0.02], [0.03, 0.07, 0.01], [0.04, 0.02, 0.08]]\n",
    "                        )\n",
    "                    )\n",
    "                    self.model.transformer_blocks[t_idx].mha.attentions[\n",
    "                        a_idx\n",
    "                    ].W_K = nn.Parameter(\n",
    "                        torch.tensor(\n",
    "                            [[0.06, 0.03, 0.09], [0.02, 0.08, 0.01], [0.07, 0.01, 0.04]]\n",
    "                        )\n",
    "                    )\n",
    "                    self.model.transformer_blocks[t_idx].mha.attentions[\n",
    "                        a_idx\n",
    "                    ].W_V = nn.Parameter(\n",
    "                        torch.tensor(\n",
    "                            [[0.05, 0.02, 0.1], [0.08, 0.01, 0.03], [0.01, 0.04, 0.07]]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # 2.2. define weights for the ANN layers\n",
    "                # self.model.transformer_blocks[t_idx].ann[0] = nn.Linear(\n",
    "                #     self.d_model, self.d_model, bias=True\n",
    "                # )\n",
    "                self.model.transformer_blocks[t_idx].ann[0].weight = nn.Parameter(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            [0.1, 0.05, 0.02],\n",
    "                            [0.03, 0.07, 0.01],\n",
    "                            [0.04, 0.02, 0.08],\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "                self.model.transformer_blocks[t_idx].ann[0].bias = nn.Parameter(\n",
    "                    torch.tensor([0.1, 0.05, 0.02])\n",
    "                )\n",
    "                # self.model.transformer_blocks[t_idx].ann[3] = nn.Linear(\n",
    "                #     self.d_model, self.d_model, bias=True\n",
    "                # )\n",
    "                self.model.transformer_blocks[t_idx].ann[3].weight = nn.Parameter(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            [0.1, 0.05, 0.02],\n",
    "                            [0.03, 0.07, 0.01],\n",
    "                            [0.04, 0.02, 0.08],\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "                self.model.transformer_blocks[t_idx].ann[3].bias = nn.Parameter(\n",
    "                    torch.tensor([0.1, 0.05, 0.02])\n",
    "                )\n",
    "\n",
    "                # 2.3. define weights for the W_O\n",
    "                # W_O must have shape (h * d_V, d_model)\n",
    "                self.model.transformer_blocks[t_idx].mha.W_O = nn.Parameter(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            [0.1, 0.05, 0.02],\n",
    "                            [0.03, 0.07, 0.01],\n",
    "                            [0.04, 0.02, 0.08],\n",
    "                            [0.06, 0.03, 0.09],\n",
    "                            [0.02, 0.08, 0.01],\n",
    "                            [0.07, 0.01, 0.04],\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # assert W_O has shape (h * d_V, d_model)\n",
    "                self.assertEqual(\n",
    "                    self.model.transformer_blocks[t_idx].mha.W_O.shape,\n",
    "                    torch.Size((self.h * self.d_v, self.d_model)),\n",
    "                )\n",
    "\n",
    "            # print model parameters\n",
    "            # print(\"\\n----------------------\")\n",
    "            # print(\"model.parameters()\")\n",
    "            # for name, param in self.model.named_parameters():\n",
    "            #     print(name, param.data)\n",
    "\n",
    "            # Now that wights are defined, let's in fact unittest the forward pass of the ClassifierEncoder\n",
    "            self.model.eval()\n",
    "            out = self.model(x)\n",
    "            print(\"\\n----------------------\")\n",
    "            print(\"out.shape: \", out.shape)\n",
    "            print(\"out\", out)\n",
    "\n",
    "            # 1. test the shape of the output\n",
    "            self.assertEqual(out.shape, torch.Size((self.batch_size, N_CLASSES)))\n",
    "\n",
    "            # 2. test the values of the output\n",
    "            expected_output = calculate_expected_output(x, self.model)\n",
    "            print(\"\\n----------------------\")\n",
    "            print(\"expected_output.shape: \", expected_output.shape)\n",
    "            print(\"expected_output\", expected_output)\n",
    "\n",
    "            self.assertTrue(torch.allclose(out, expected_output, atol=1e-3))\n",
    "\n",
    "\n",
    "test = ClassifierEncoderTest()\n",
    "test.setUp()\n",
    "test.test_forward()\n",
    "# RUN_UNIT_TESTS and unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Let's try a classification problem using the CLassification Encoder\n",
    "* See the file: Fine-Tuning (Intermediate)/Fine-Tunning Sentiment Custom Dataset + Labels.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, lets make a dataset with cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.read_csv(\"../Fine-Tuning (Intermediate)/AirlineTweets.csv\")\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_[[\"airline_sentiment\", \"text\"]].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text  target\n",
       "0           neutral                @VirginAmerica What @dhepburn said.       2\n",
       "1          positive  @VirginAmerica plus you've added commercials t...       1\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...       2\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...       0\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_map = {\n",
    "    \"positive\": 1,\n",
    "    \"negative\": 0,\n",
    "    \"neutral\": 2,\n",
    "}\n",
    "df[\"target\"] = df[\"airline_sentiment\"].map(target_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text  target\n",
       "1          positive  @VirginAmerica plus you've added commercials t...       1\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...       0\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...       0\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...       0\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX...       1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df[df[\"target\"] != 2]\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence,label\n",
      "@VirginAmerica plus you've added commercials to the experience... tacky.,1\n",
      "\"@VirginAmerica it's really aggressive to blast obnoxious \"\"entertainment\"\" in your guests' faces &amp; they have little recourse\",0\n",
      "@VirginAmerica and it's a really big bad thing about it,0\n",
      "\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\n",
      "it's really the only bad thing about flying VA\",0\n",
      "\"@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)\",1\n",
      "\"@virginamerica Well, I didn't…but NOW I DO! :-D\",1\n",
      "\"@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.\",1\n",
      "@VirginAmerica I &lt;3 pretty graphics. so much better than minimal iconography. :D,1\n"
     ]
    }
   ],
   "source": [
    "df2 = df_filtered[['text', 'target']]\n",
    "# Not documented info: targets must have the column name label\n",
    "# sentence may have other names, but not label\n",
    "df2.columns = ['sentence', 'label']\n",
    "df2.to_csv(\"data.csv\", index=False)\n",
    "!head data.csv\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eada46eb6ad4489faef576ecd2979cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ef1d8d08a64a32afee171326994199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f6cf3c7f3f459ea9bfb07c3143c085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"csv\", data_files=\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 11541\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "def splitTrainTestValidation(dataset: Dataset, valid_size=0.1, test_size=0.1):\n",
    "    len_valid = int(len(dataset) * valid_size)\n",
    "    len_test = int(len(dataset) * test_size)\n",
    "\n",
    "    splited: DatasetDict = dataset.train_test_split(\n",
    "        len_valid + len_test, shuffle=False, seed=42\n",
    "    )\n",
    "    splited[\"validation\"] = splited[\"test\"]\n",
    "    del splited[\"test\"]\n",
    "\n",
    "    splited_2 = splited[\"validation\"].train_test_split(len_test, shuffle=True, seed=42)\n",
    "    splited[\"validation\"] = splited_2[\"train\"]\n",
    "    splited[\"test\"] = splited_2[\"test\"]\n",
    "\n",
    "    return splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = raw_dataset['train'].train_test_split(test_size=.3, seed=42)\n",
    "split = splitTrainTestValidation(raw_dataset[\"train\"], valid_size=0.1, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 9233\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 1154\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 1154\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=T\n",
    "    )\n",
    "\n",
    "\n",
    "# tokenizer(\"This is an example\", truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56da2a8fec64e5980a1a0afa6c73478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9233 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ea0f01792d4467ac758c1de7c2f66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede4d64511c94392907b2e9aeec052a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 9233\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1154\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1154\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = split.map(tokenize_fn, batched=False)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bad5eac819468c822502233c4d3376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f9746b592e4bbbbdd92e56ee56ffbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 9233\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 853\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 844\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Little notation here:\n",
    "# token is an int from the tokenizer\n",
    "# idx is our index, to use in our embedding\n",
    "\n",
    "token2idx = {0: 0}\n",
    "idx2token = {}\n",
    "\n",
    "all_tokens = [\n",
    "    element\n",
    "    for list_ids in tokenized_datasets[\"train\"][\"input_ids\"]\n",
    "    for element in list_ids\n",
    "]\n",
    "all_tokens = list(set(all_tokens))\n",
    "\n",
    "token_index = 0\n",
    "for token in all_tokens:\n",
    "    if token not in token2idx:\n",
    "        token2idx[token] = token_index\n",
    "        idx2token[token_index] = token\n",
    "        token_index += 1\n",
    "\n",
    "\n",
    "def filterSplit(splited_dataset):\n",
    "    \"\"\"For valid and test datasets, get only those which all inpu_ids is in splited_dataset['train']\"\"\"\n",
    "\n",
    "    for split in [\"validation\", \"test\"]:\n",
    "        # Filter the splited_dataset[split] to only keep the ids which are in splited_dataset['train']\n",
    "        splited_dataset[split] = splited_dataset[split].filter(\n",
    "            lambda x: all(token in token2idx for token in x[\"input_ids\"])\n",
    "        )\n",
    "\n",
    "    return splited_dataset\n",
    "\n",
    "\n",
    "filtered_datasets = filterSplit(tokenized_datasets)\n",
    "filtered_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos construir nosso dicionário de tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d01ab0f8c24f69a06ca483c8ce50e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9233 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debdd568bc48492caa05d4751f952143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549b88b814634b6785f18000cf73819f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/844 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask', 'input_idx'],\n",
       "        num_rows: 9233\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask', 'input_idx'],\n",
       "        num_rows: 853\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask', 'input_idx'],\n",
       "        num_rows: 844\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makeIndex(batch):\n",
    "    batch[\"input_idx\"] = [token2idx[single] for single in batch[\"input_ids\"]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "data = filtered_datasets.map(makeIndex, batched=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assert all same length\n",
    "list(set([len(l_idx) for l_idx in data[\"train\"][\"input_idx\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9233, 80])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = torch.Tensor(data[\"train\"][\"input_idx\"]).type(torch.int32)\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "target_train = torch.Tensor(data[\"train\"][\"label\"])\n",
    "\n",
    "# make one_hot\n",
    "target_train = F.one_hot(target_train.to(torch.int64), num_classes=N_CLASSES).float()\n",
    "\n",
    "target_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([853, 80])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid = torch.Tensor(data[\"validation\"][\"input_idx\"]).type(torch.int32)\n",
    "data_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_valid = torch.Tensor(data[\"validation\"][\"label\"])\n",
    "\n",
    "# make one_hot\n",
    "target_valid = F.one_hot(target_valid.to(torch.int64), num_classes=N_CLASSES).float()\n",
    "\n",
    "target_valid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([844, 80])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = torch.Tensor(data[\"test\"][\"input_idx\"]).type(torch.int32)\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test = torch.Tensor(data[\"test\"][\"label\"])\n",
    "\n",
    "# make one_hot\n",
    "target_test = F.one_hot(target_test.to(torch.int64), num_classes=N_CLASSES).float()\n",
    "\n",
    "target_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's train our model! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [1152/9233 (12%)]\tLoss: 0.003315\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35878/3787181718.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35878/3787181718.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, epoch)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35878/2378210186.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35878/2554869558.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35878/3504943944.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAttention\u001b[0m \u001b[0mweights\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \"\"\"\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mattention_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         attention_weights = torch.stack(\n\u001b[1;32m     22\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35878/3504943944.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAttention\u001b[0m \u001b[0mweights\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \"\"\"\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mattention_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         attention_weights = torch.stack(\n\u001b[1;32m     22\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35878/2149474226.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# (batch, T, d_model) x (d_model, d_K) -> (batch, T, d_K)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_K\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_Q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_V\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35878/3548129089.py\u001b[0m in \u001b[0;36mbatched_matmul\u001b[0;34m(tensor_3d, tensor_2d)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatched_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mW_repeated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_repeated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35878/3548129089.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(x, n)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# for example, if shape of x is (3, 4, 8), shapee must be (n, 1, 1, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     tuple_ones = tuple(\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtuple_ones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "log_interval = 10\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader, criterion, verbose=False):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in eval_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            if type(criterion) == torch.nn.modules.loss.NLLLoss:\n",
    "                loss += criterion(output, target.argmax(dim=-1)).item()\n",
    "            else:\n",
    "                loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            target_ = target.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target_).sum()\n",
    "\n",
    "    # loss /= len(eval_loader.dataset)\n",
    "    verbose and print(\n",
    "        \"\\nAverage loss: {:.6f}, Accuracy: {}/{} ({:.2f}%)\\n\".format(\n",
    "            loss,\n",
    "            correct,\n",
    "            len(eval_loader.dataset),\n",
    "            correct / len(eval_loader.dataset) * 100,\n",
    "        )\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    total_loss = 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        if type(criterion) == torch.nn.modules.loss.NLLLoss:\n",
    "            loss = criterion(output, target.argmax(dim=-1))\n",
    "        else:\n",
    "            loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (batch_idx + 1) / len(data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # valid_loss = evaluate(model, validation_loader, criterion)\n",
    "        # if valid_loss < best_loss:\n",
    "        #     best_loss = valid_loss\n",
    "        #     torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain loss (avg): {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    avg_loss,\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]\n",
    "\n",
    "\n",
    "model = ClassifierEncoder(vocab_size=len(token2idx))\n",
    "\n",
    "train_dataset = MyDataset(data_train, target_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "valid_dataset = MyDataset(data_valid, target_valid)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = MyDataset(data_train, target_train)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# criterion = torch.nn.NLLLoss()\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "best_loss = np.inf\n",
    "valid_losses = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def plotLosses(losses):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    valid_losses = []\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train(model, train_loader, optimizer, criterion, epoch)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, verbose=True)\n",
    "        valid_losses.append(valid_loss)\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"model.pth\")\n",
    "        print(f\"Epoch: {epoch}\\tloss: {valid_loss}\")\n",
    "        \n",
    "        # Plotting\n",
    "        plotLosses(valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit tests for transformer and attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestTensorFunctions(unittest.TestCase):\n",
    "    def test_repeat(self):\n",
    "        x = torch.tensor([1, 2, 3])\n",
    "        n = 2\n",
    "        result = repeat(x, n)\n",
    "        expected = torch.tensor([[1, 2, 3], [1, 2, 3]])\n",
    "        self.assertTrue(torch.equal(result, expected))\n",
    "\n",
    "    def test_batched_matmul(self):\n",
    "        tensor_3d = torch.randn(10, 3, 4)\n",
    "        tensor_2d = torch.randn(4, 5)\n",
    "        result = batched_matmul(tensor_3d, tensor_2d)\n",
    "        expected = torch.bmm(tensor_3d, tensor_2d.unsqueeze(0).repeat((10, 1, 1)))\n",
    "        self.assertTrue(torch.allclose(result, expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class EmbeddingLayerTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Set up your embedding layer or load your model\n",
    "        self.vocab_size = 10\n",
    "        self.d_model = 8\n",
    "        self.batch_size = 2\n",
    "        self.t = 4\n",
    "        self.model = Embedding(self.vocab_size, self.d_model)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def test_embedding_layer_forward(self):\n",
    "        # Define input tensor (batch_size=2, sequence_length=4)\n",
    "        x = torch.tensor([[1, 3, 5, 7], [0, 2, 4, 6]])\n",
    "\n",
    "        # Manually set model weights for the embedding matrix\n",
    "        self.model.embedding.data = torch.tensor(\n",
    "            [\n",
    "                [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "                [0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6],\n",
    "                [1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4],\n",
    "                [2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2],\n",
    "                [3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0],\n",
    "                [4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8],\n",
    "                [4.9, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6],\n",
    "                [5.7, 5.8, 5.9, 6.0, 6.1, 6.2, 6.3, 6.4],\n",
    "                [6.5, 6.6, 6.7, 6.8, 6.9, 7.0, 7.1, 7.2],\n",
    "                [7.3, 7.4, 7.5, 7.6, 7.7, 7.8, 7.9, 8.0],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Calculate the expected output manually\n",
    "        expected_output = torch.tensor(\n",
    "            [\n",
    "                [\n",
    "                    [\n",
    "                        [0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6],\n",
    "                        [2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2],\n",
    "                        [4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8],\n",
    "                        [5.7, 5.8, 5.9, 6.0, 6.1, 6.2, 6.3, 6.4],\n",
    "                    ],\n",
    "                    [\n",
    "                        [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "                        [1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4],\n",
    "                        [3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0],\n",
    "                        [4.9, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6],\n",
    "                    ],\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Check if the shape of the expected output matches\n",
    "        self.assertEqual(expected_output.shape, expected_output.shape)\n",
    "\n",
    "        # Check if the values of the expected output tensor match\n",
    "        self.assertTrue(torch.allclose(expected_output, expected_output, atol=1e-6))\n",
    "\n",
    "    def test_embedding_layer_loss(self):\n",
    "        # Define input tensors and target tensors\n",
    "        input_tensor = torch.randint(\n",
    "            0, self.vocab_size, (self.batch_size, self.t)\n",
    "        ).long()\n",
    "        target_tensor = torch.randn(self.batch_size, self.t, self.d_model)\n",
    "\n",
    "        # Forward pass through the embedding layer\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.criterion(output, target_tensor)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param.grad != 0), \"No gradients in the parameters\"\n",
    "            )\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )\n",
    "\n",
    "\n",
    "# This will run the test in the notebook\n",
    "# RUN_UNIT_TESTS and unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAttention(unittest.TestCase):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        unittest (_type_): unit test for a conjuntion linear layer\n",
    "            plus the attention block\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class AttentionTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 2\n",
    "        self.d_k = 3\n",
    "        self.d_v = 3\n",
    "        self.T = 2\n",
    "        self.d_model = 4\n",
    "\n",
    "        # Set up your attention mechanism or load your model\n",
    "        self.model = Attention(self.d_k, self.d_v, self.d_model)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def test_attention_forward(self):\n",
    "        # Define input tensor (batch_size=2, sequence_length=2, d_model=4)\n",
    "        x = torch.tensor(\n",
    "            [\n",
    "                [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]],\n",
    "                [[9.0, 10.0, 11.0, 12.0], [13.0, 14.0, 15.0, 16.0]],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Manually set model weights for W_K, W_Q, and W_V\n",
    "        self.model.W_K.data = torch.tensor(\n",
    "            [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.model.W_Q.data = torch.tensor(\n",
    "            [[1.2, 1.1, 1.0], [0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.model.W_V.data = torch.tensor(\n",
    "            [[0.5, 0.6, 0.7], [0.8, 0.9, 1.0], [1.1, 1.2, 1.3], [1.4, 1.5, 1.6]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "\n",
    "        # Expected output based on calculations from the attention mechanism\n",
    "        expected_output, _ = self.model(x)\n",
    "\n",
    "        # Calculate the expected output manually\n",
    "        # Calculate Q, K, and V using learned weights\n",
    "        Q = torch.matmul(x, self.model.W_Q)\n",
    "        K = torch.matmul(x, self.model.W_K)\n",
    "        V = torch.matmul(x, self.model.W_V)\n",
    "\n",
    "        # Calculate the attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(1, 2)) / (self.d_k**0.5)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Apply attention weights to V to get the output\n",
    "        expected_output_manual = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Check if the shape of the expected output matches\n",
    "        self.assertEqual(expected_output.shape, expected_output_manual.shape)\n",
    "\n",
    "        # Check if the values of the expected output tensor match\n",
    "        self.assertTrue(\n",
    "            torch.allclose(expected_output, expected_output_manual, atol=1e-6)\n",
    "        )\n",
    "\n",
    "    def test_attention_loss(self):\n",
    "        # Test the loss function associated with the attention mechanism\n",
    "        # Define input tensors and target tensors\n",
    "\n",
    "        input_tensor = torch.rand(\n",
    "            (self.batch_size, self.T, self.d_model), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # Forward pass through the attention mechanism\n",
    "        output, _ = self.model(input_tensor)\n",
    "\n",
    "        # Create dummy target tensor (replace with your actual target tensor)\n",
    "        target = torch.rand_like(output, requires_grad=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.criterion(output, target)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if gradients are computed\n",
    "        self.assertTrue(\n",
    "            input_tensor.grad is not None, \"No gradients in the input tensor\"\n",
    "        )\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )\n",
    "\n",
    "\n",
    "# This will run the test in the notebook\n",
    "# RUN_UNIT_TESTS and unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[159.9200, 174.0800, 188.2400, 202.4000],\n",
      "         [159.9200, 174.0800, 188.2400, 202.4000]],\n",
      "\n",
      "        [[344.5600, 375.0400, 405.5200, 436.0000],\n",
      "         [344.5600, 375.0400, 405.5200, 436.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# Code by GPT4 to calculate the multihead attention output\n",
    "\n",
    "import torch\n",
    "\n",
    "# Define the parameters\n",
    "d_k = 3\n",
    "d_v = 3\n",
    "d_model = 4\n",
    "h = 2\n",
    "batch_size = 2\n",
    "t = 2\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = torch.tensor(\n",
    "    [\n",
    "        [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]],\n",
    "        [[9.0, 10.0, 11.0, 12.0], [13.0, 14.0, 15.0, 16.0]],\n",
    "    ],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Define the weights\n",
    "W_K = torch.tensor(\n",
    "    [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "W_Q = torch.tensor(\n",
    "    [[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 1.0], [1.1, 1.2, 1.3]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "W_V = torch.tensor(\n",
    "    [[0.3, 0.4, 0.5], [0.6, 0.7, 0.8], [0.9, 1.0, 1.1], [1.2, 1.3, 1.4]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "W_O = torch.tensor(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4],\n",
    "        [0.5, 0.6, 0.7, 0.8],\n",
    "        [0.9, 1.0, 1.1, 1.2],\n",
    "        [1.3, 1.4, 1.5, 1.6],\n",
    "        [1.7, 1.8, 1.9, 2.0],\n",
    "        [2.1, 2.2, 2.3, 2.4],\n",
    "    ],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Initialize tensors for K, Q, V for each head\n",
    "Ks, Qs, Vs = [], [], []\n",
    "\n",
    "# Compute K, Q, V for each head\n",
    "for _ in range(h):\n",
    "    K = torch.matmul(input_tensor, W_K)\n",
    "    Q = torch.matmul(input_tensor, W_Q)\n",
    "    V = torch.matmul(input_tensor, W_V)\n",
    "    Ks.append(K)\n",
    "    Qs.append(Q)\n",
    "    Vs.append(V)\n",
    "\n",
    "# Initialize a tensor for the concatenated results\n",
    "concatenated_results = torch.zeros(batch_size, t, h * d_v)\n",
    "\n",
    "# Calculate the attention and concatenate results for each head\n",
    "for a_idx in range(h):\n",
    "    # Scaled dot product of Q and K\n",
    "    attention_scores = torch.matmul(Qs[a_idx], Ks[a_idx].transpose(-2, -1)) / (\n",
    "        d_k**0.5\n",
    "    )\n",
    "    attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    # Weighted sum of V\n",
    "    head_output = torch.matmul(attention_weights, Vs[a_idx])\n",
    "\n",
    "    # Concatenate results\n",
    "    concatenated_results[:, :, a_idx * d_v : (a_idx + 1) * d_v] = head_output\n",
    "\n",
    "# Apply the final linear layer W_O\n",
    "expected_output = torch.matmul(concatenated_results, W_O)\n",
    "\n",
    "print(expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Initialize the multi-head attention model\n",
    "        self.model = MultiHeadAttention(self.h, self.d_model, self.d_k, self.d_v)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Manually setting the weights for the test\n",
    "        for head in self.model.attentions:\n",
    "            # These weights should be carefully chosen to ensure a predictable output\n",
    "            head.W_K = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]\n",
    "                )\n",
    "            )\n",
    "            head.W_Q = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 1.0], [1.1, 1.2, 1.3]]\n",
    "                )\n",
    "            )\n",
    "            head.W_V = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [[0.3, 0.4, 0.5], [0.6, 0.7, 0.8], [0.9, 1.0, 1.1], [1.2, 1.3, 1.4]]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output linear layer weights\n",
    "        self.model.W_O = nn.Parameter(\n",
    "            torch.tensor(\n",
    "                [\n",
    "                    [0.1, 0.2, 0.3, 0.4],\n",
    "                    [0.5, 0.6, 0.7, 0.8],\n",
    "                    [0.9, 1.0, 1.1, 1.2],\n",
    "                    [1.3, 1.4, 1.5, 1.6],\n",
    "                    [1.7, 1.8, 1.9, 2.0],\n",
    "                    [2.1, 2.2, 2.3, 2.4],\n",
    "                ],\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Shape of W_O, according to the paper \"Attention Is All You Need\"\n",
    "        self.assertEqual(list(self.model.W_O.shape), [self.h * self.d_v, self.d_model])\n",
    "\n",
    "    def test_multi_head_attention_forward(self):\n",
    "        # Define a specific input tensor (batch_size, t, d_model)\n",
    "        input_tensor = torch.tensor(\n",
    "            [\n",
    "                [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]],\n",
    "                [[9.0, 10.0, 11.0, 12.0], [13.0, 14.0, 15.0, 16.0]],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Expected output tensor based on the input and the model weights\n",
    "        # This tensor should be calculated manually based on the model's operations\n",
    "        expected_output = torch.Tensor(\n",
    "            [\n",
    "                [\n",
    "                    [159.9200, 174.0800, 188.2400, 202.4000],\n",
    "                    [159.9200, 174.0800, 188.2400, 202.4000],\n",
    "                ],\n",
    "                [\n",
    "                    [344.5600, 375.0400, 405.5200, 436.0000],\n",
    "                    [344.5600, 375.0400, 405.5200, 436.0000],\n",
    "                ],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Run the forward pass\n",
    "        output, _ = self.model(input_tensor)\n",
    "\n",
    "        # Check the shape of the output tensor\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.t, self.d_model))\n",
    "\n",
    "        # Check the values of the output tensor\n",
    "        self.assertTrue(torch.allclose(output, expected_output, atol=1e-4))\n",
    "\n",
    "    def test_multi_head_attention_loss(self):\n",
    "        # Test the loss function associated with the multi-head attention mechanism\n",
    "        # Define input tensors and target tensors\n",
    "\n",
    "        input_tensor = torch.rand(\n",
    "            (self.batch_size, self.t, self.d_model), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # Forward pass through the multi-head attention mechanism\n",
    "        output, _ = self.model(input_tensor)\n",
    "\n",
    "        # Create dummy target tensor (replace with your actual target tensor)\n",
    "        target = torch.rand_like(output, requires_grad=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.criterion(output, target)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if gradients are computed\n",
    "        self.assertTrue(\n",
    "            input_tensor.grad is not None, \"No gradients in the input tensor\"\n",
    "        )\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )\n",
    "\n",
    "\n",
    "# This will run the test in the notebook\n",
    "# RUN_UNIT_TESTS and unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_output(x, attentions, W_O):\n",
    "    batch_size, T, d_model = x.shape\n",
    "    h = len(attentions)\n",
    "    d_k, d_v = attentions[0].W_K.shape[1], attentions[0].W_V.shape[1]\n",
    "\n",
    "    # Initialize the tensor to hold the concatenated outputs from each head\n",
    "    concatenated_heads = torch.zeros((batch_size, T, h * d_v))\n",
    "\n",
    "    for i in range(h):\n",
    "        W_K, W_Q, W_V = attentions[i].W_K, attentions[i].W_Q, attentions[i].W_V\n",
    "\n",
    "        # Calculate K, Q, V matrices\n",
    "        K = x @ W_K\n",
    "        Q = x @ W_Q\n",
    "        V = x @ W_V\n",
    "\n",
    "        # Calculate attention scores and apply softmax (simplified here)\n",
    "        attention_scores = (Q @ K.transpose(-2, -1)) / (d_k**0.5)\n",
    "        # In a real scenario, apply softmax to attention_scores here\n",
    "\n",
    "        # Calculate the weighted sum of V\n",
    "        weighted_sum = attention_scores @ V\n",
    "\n",
    "        # Concatenate results from each head\n",
    "        concatenated_heads[:, :, i * d_v : (i + 1) * d_v] = weighted_sum\n",
    "\n",
    "    # Multiply with W_O\n",
    "    expected_output = concatenated_heads @ W_O\n",
    "    return expected_output\n",
    "\n",
    "\n",
    "class TransformerBlockTest(unittest.TestCase):\n",
    "    def calculate_attention_output(self, x, W_K, W_Q, W_V, d_k):\n",
    "        \"\"\"\n",
    "        Calculate the output of a single head attention.\n",
    "        \"\"\"\n",
    "        K = x @ W_K\n",
    "        Q = x @ W_Q\n",
    "        V = x @ W_V\n",
    "\n",
    "        attention_scores = (Q @ K.transpose(-2, -1)) / (d_k**0.5)\n",
    "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = attention_scores @ V\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "    def calculate_expected_output(\n",
    "        self, x, W_Ks, W_Qs, W_Vs, W_O, linear_weights, linear_biases\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculate the expected output of the TransformerBlock.\n",
    "        \"\"\"\n",
    "        batch_size, T, d_model = x.shape\n",
    "        h = len(W_Ks)\n",
    "        d_k, d_v = W_Ks[0].shape[1], W_Vs[0].shape[1]\n",
    "\n",
    "        # Step 1: Multi-head attention\n",
    "        concatenated_heads = torch.zeros((batch_size, T, h * d_v))\n",
    "        for i in range(h):\n",
    "            attention_output = self.calculate_attention_output(\n",
    "                x, W_Ks[i], W_Qs[i], W_Vs[i], d_k\n",
    "            )\n",
    "            concatenated_heads[:, :, i * d_v : (i + 1) * d_v] = attention_output\n",
    "\n",
    "        multihead_output = concatenated_heads @ W_O\n",
    "\n",
    "        # Step 2: Apply LayerNorm (simplified version)\n",
    "        norm_output = F.layer_norm(multihead_output, multihead_output.shape[1:])\n",
    "\n",
    "        # Step 3: ANN layers (assuming two linear layers with ReLU and Dropout in between)\n",
    "        ann_output = F.linear(norm_output, linear_weights[0], linear_biases[0])\n",
    "        ann_output = F.dropout(ann_output, p=0.1)\n",
    "        ann_output = F.relu(ann_output)\n",
    "        ann_output = F.linear(ann_output, linear_weights[1], linear_biases[1])\n",
    "\n",
    "        return ann_output\n",
    "\n",
    "    def setUp(self):\n",
    "        self.batch_size = 2\n",
    "        self.d_k = 3\n",
    "        self.d_v = 3\n",
    "        self.t = 4\n",
    "        self.d_model = 3\n",
    "        self.h = 2\n",
    "        # Set up your Transformer block model\n",
    "        self.model = TransformerBlock(\n",
    "            d_K=self.d_k, d_V=self.d_v, d_model=self.d_model, h=self.h\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "def test_transformer_block_forward(self):\n",
    "    # Define the input tensor\n",
    "    x = torch.tensor(\n",
    "        [\n",
    "            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]],\n",
    "            [\n",
    "                [13.0, 14.0, 15.0],\n",
    "                [16.0, 17.0, 18.0],\n",
    "                [19.0, 20.0, 21.0],\n",
    "                [22.0, 23.0, 24.0],\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Manually set and collect the weights for each Attention layer\n",
    "    W_Ks = []\n",
    "    W_Qs = []\n",
    "    W_Vs = []\n",
    "    for i, attention in enumerate(self.model.mha.attentions):\n",
    "        W_K = torch.tensor(\n",
    "            [\n",
    "                [0.1 * (i + 1), 0.2 * (i + 1), 0.3 * (i + 1)],\n",
    "                [0.4 * (i + 1), 0.5 * (i + 1), 0.6 * (i + 1)],\n",
    "                [0.7 * (i + 1), 0.8 * (i + 1), 0.9 * (i + 1)],\n",
    "            ]\n",
    "        )\n",
    "        W_Q = torch.tensor(\n",
    "            [\n",
    "                [0.9 * (i + 1), 0.8 * (i + 1), 0.7 * (i + 1)],\n",
    "                [0.6 * (i + 1), 0.5 * (i + 1), 0.4 * (i + 1)],\n",
    "                [0.3 * (i + 1), 0.2 * (i + 1), 0.1 * (i + 1)],\n",
    "            ]\n",
    "        )\n",
    "        W_V = torch.tensor(\n",
    "            [\n",
    "                [0.1 * (i + 1), 0.1 * (i + 1), 0.1 * (i + 1)],\n",
    "                [0.2 * (i + 1), 0.2 * (i + 1), 0.2 * (i + 1)],\n",
    "                [0.3 * (i + 1), 0.3 * (i + 1), 0.3 * (i + 1)],\n",
    "            ]\n",
    "        )\n",
    "        attention.W_K.data = W_K\n",
    "        attention.W_Q.data = W_Q\n",
    "        attention.W_V.data = W_V\n",
    "        W_Ks.append(W_K)\n",
    "        W_Qs.append(W_Q)\n",
    "        W_Vs.append(W_V)\n",
    "\n",
    "    # Set and collect weights for W_O in MultiHeadAttention\n",
    "    W_O = torch.tensor(\n",
    "        [\n",
    "            [0.1, 0.2, 0.3],\n",
    "            [0.4, 0.5, 0.6],\n",
    "            [0.7, 0.8, 0.9],\n",
    "            [0.9, 0.8, 0.7],\n",
    "            [0.6, 0.5, 0.4],\n",
    "            [0.3, 0.2, 0.1],\n",
    "        ]\n",
    "    )\n",
    "    self.model.mha.W_O.data = W_O\n",
    "\n",
    "    # Collect weights and biases for ANN layers (Assuming two linear layers)\n",
    "    linear_weights = [self.model.ann[0].weight, self.model.ann[3].weight]\n",
    "    linear_biases = [self.model.ann[0].bias, self.model.ann[3].bias]\n",
    "\n",
    "    # Calculate the expected output tensor\n",
    "    expected_output = self.calculate_expected_output(\n",
    "        x, W_Ks, W_Qs, W_Vs, W_O, linear_weights, linear_biases\n",
    "    )\n",
    "\n",
    "    # Forward pass\n",
    "    output, _ = self.model(x)\n",
    "\n",
    "    # Check the shape of the output tensor\n",
    "    self.assertEqual(output.shape, expected_output.shape)\n",
    "\n",
    "    # Check the values of the output tensor\n",
    "    self.assertTrue(torch.allclose(output, expected_output, atol=1e-6))\n",
    "\n",
    "    def test_transformer_block_loss(self):\n",
    "        # Test the loss function associated with the Transformer block\n",
    "        # Define input tensors and target tensors\n",
    "\n",
    "        input_tensor = torch.rand(\n",
    "            (self.batch_size, self.t, self.d_model), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # Forward pass through the Transformer block\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # Create dummy target tensor (replace with your actual target tensor)\n",
    "        target = torch.rand_like(output, requires_grad=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.criterion(output, target)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if gradients are computed\n",
    "        self.assertTrue(\n",
    "            input_tensor.grad is not None, \"No gradients in the input tensor\"\n",
    "        )\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )\n",
    "\n",
    "\n",
    "# This will run the test in the notebook\n",
    "# RUN_UNIT_TESTS and unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..E..EE..\n",
      "======================================================================\n",
      "ERROR: test_forward (__main__.ClassifierEncoderTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35878/362621016.py\", line 344, in test_forward\n",
      "    expected_output = calculate_expected_output(x, self.model)\n",
      "TypeError: calculate_expected_output() missing 1 required positional argument: 'W_O'\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_multi_head_attention_forward (__main__.MultiHeadAttentionTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35878/417674948.py\", line 4, in setUp\n",
      "    self.model = MultiHeadAttention(self.h, self.d_model, self.d_k, self.d_v)\n",
      "AttributeError: 'MultiHeadAttentionTest' object has no attribute 'h'\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_multi_head_attention_loss (__main__.MultiHeadAttentionTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35878/417674948.py\", line 4, in setUp\n",
      "    self.model = MultiHeadAttention(self.h, self.d_model, self.d_k, self.d_v)\n",
      "AttributeError: 'MultiHeadAttentionTest' object has no attribute 'h'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 0.012s\n",
      "\n",
      "FAILED (errors=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------\n",
      "x.shape: torch.Size([3, 4])\n",
      "x: tensor([[0, 1, 2, 3],\n",
      "        [1, 4, 5, 0],\n",
      "        [4, 5, 2, 3]])\n",
      "\n",
      "----------------------\n",
      "out.shape:  torch.Size([3, 2])\n",
      "out tensor([[0.2784, 0.7216],\n",
      "        [0.2604, 0.7396],\n",
      "        [0.2784, 0.7216]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fb6d759cfd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will run the test in the notebook\n",
    "RUN_UNIT_TESTS and unittest.main(argv=[\"\"], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
