{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters Here\n",
    "BATCH_SIZE = 4\n",
    "T = 4\n",
    "D_K = 2\n",
    "D_V = 2\n",
    "D_MODEL = 2\n",
    "H = 8\n",
    "VOCAB_SIZE = 10\n",
    "N_MHA_BLOCKS_ENCODER = 6\n",
    "N_CLASSES = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "# Test softmax x axis:\n",
    "matrix = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "softmaxed_matrix = F.softmax(matrix, dim=1)\n",
    "\n",
    "print(softmaxed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(x: torch.Tensor, n: int):\n",
    "    # make shape (n, 1, 1, ...) --> quantity of 1's must be len(x.shape)\n",
    "    # for example, if shape of x is (3, 4, 8), shapee must be (n, 1, 1, 1)\n",
    "    tuple_ones = tuple(\n",
    "        (torch.tensor(x.shape) / torch.tensor(x.shape)).numpy().astype(int)\n",
    "    )\n",
    "    # print((n, *tuple_ones))\n",
    "    return x.unsqueeze(0).repeat((n, *tuple_ones))\n",
    "\n",
    "\n",
    "def batched_matmul(x_batched, W):\n",
    "    # # Assuming x_batched.shape == (batch_size, T, d_model)\n",
    "    # # and W.shape == (d_model, d)\n",
    "    # batch_size, T, d_model = x_batched.shape\n",
    "    # d = W.shape[1]\n",
    "\n",
    "    # # Reshape x_batched to (batch_size * T, d_model)\n",
    "    # x_reshaped = x_batched.reshape(-1, d_model)\n",
    "\n",
    "    # # Perform matrix multiplication\n",
    "    # result = torch.matmul(x_reshaped, W)\n",
    "\n",
    "    # # Reshape the result back to (batch_size, T, d)\n",
    "    # result = result.reshape(batch_size, T, d)\n",
    "\n",
    "    # return result\n",
    "\n",
    "    # batch_size = x_batched.shape[0]\n",
    "    # W_repeated = W.unsqueeze(0).repeat((batch_size, 1, 1))\n",
    "    W_repeated = repeat(W, n=x_batched.shape[0])\n",
    "    print(x_batched.shape, W_repeated.shape)\n",
    "    return torch.bmm(x_batched, W_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" Convention from: https://www.udemy.com/course/data-science-transformers-nlp/learn/lecture/32255056#overview\n",
    "    In our convention, K, Q and V are learneable, different from the \"Attention is all you need\" paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, T: int, d_K, d_V, d_model: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # define a torch 2d tensor initialized normally\n",
    "        self.W_K = torch.normal(mean=0, std=0.01, size=(d_model, d_K), requires_grad=True)\n",
    "        self.W_Q = torch.normal(mean=0, std=0.01, size=(d_model, d_K), requires_grad=True)\n",
    "        self.W_V = torch.normal(mean=0, std=0.01, size=(d_model, d_V), requires_grad=True)\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Shapes:\n",
    "        # W_K (d_model, d_K)\n",
    "        # x is a 3d tensor (batch x T x d_model)\n",
    "\n",
    "        # W_K.T ->  (1, d_k x d_model)\n",
    "        # x ->      (batch, T, d_model)\n",
    "        K = batched_matmul(x, self.W_K)\n",
    "        Q = batched_matmul(x, self.W_Q)\n",
    "        V = batched_matmul(x, self.W_V)\n",
    "\n",
    "        # (batch, T, d_model) x (batch, d_model, d_k) -> (batch, T, d_k)\n",
    "        result = torch.bmm(Q, K.transpose(1, 2)) / (K.shape[-1] ** 0.5)\n",
    "        if self.mask:\n",
    "            result = batched_matmul(result, self.mask)\n",
    "        result = F.softmax(result, dim=-1)\n",
    "        result = torch.bmm(result, V)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([4, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "att = Attention(T=T, d_K=D_K, d_V=D_V, d_model=D_MODEL)\n",
    "x = torch.normal(mean=0, std=0.01, size=(BATCH_SIZE, T, D_MODEL))\n",
    "\n",
    "att_result = att.forward(x)\n",
    "assert att_result.shape == (BATCH_SIZE, T, D_V)\n",
    "print(att.W_K.shape)\n",
    "print(att.W_Q.shape)\n",
    "print(att.W_V.shape)\n",
    "print(att_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, h: int, T=T, d_K=D_K, d_V=D_V, d_model=D_MODEL, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.h = h\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [Attention(T=T, d_K=d_K, d_model=d_model, d_V=d_V) for _ in range(h)]\n",
    "        )\n",
    "        self.W_O = torch.normal(0, 0.1, size=(h * d_V, d_model), requires_grad=True)\n",
    "        self.T = T\n",
    "        self.d_V = d_V\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        attention_results = []\n",
    "        \n",
    "        for attention in self.attentions:\n",
    "            attention_result = attention(x)\n",
    "            attention_results.append(attention_result)\n",
    "\n",
    "        concatenated = torch.concat(attention_results, dim=-1)\n",
    "        return batched_matmul(concatenated, self.W_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 16]) torch.Size([4, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(h=H, T=T, d_K=D_K, d_V=D_V, d_model=D_MODEL)\n",
    "assert mha(x).shape == (BATCH_SIZE, T, D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 2]) torch.Size([4, 2, 2])\n",
      "torch.Size([4, 4, 16]) torch.Size([4, 16, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, T=T, d_K=D_K, d_V=D_V, d_model=D_MODEL, h=H, dropout=0.1, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mha = MultiHeadAttention(h, T=T, d_K=d_K, d_V=d_V, d_model=d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, 2),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x = self.mha(x)\n",
    "        # x = self.layer_norm(x)\n",
    "        x = self.layer_norm(x + self.mha(x))\n",
    "        x = self.layer_norm(x + self.ann(x))\n",
    "        return x\n",
    "        \n",
    "transformerBlock = TransformerBlock()\n",
    "transformerBlock(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
       "        [ 0.9093, -0.4161,  0.0200,  0.9998]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PositionalEncoding(L: int, d_model):\n",
    "    encodings = torch.zeros(size=(L, d_model), requires_grad=False)\n",
    "    counter = 0\n",
    "    for pos in range(L):\n",
    "        for i in range((d_model // 2) + 1):\n",
    "            if 2 * i < d_model:\n",
    "                counter += 1\n",
    "                encodings[pos, 2 * i] = torch.sin(\n",
    "                    pos / torch.tensor(10000).pow(2 * i / d_model)\n",
    "                )\n",
    "            if 2 * i + 1 < d_model:\n",
    "                counter += 1\n",
    "                encodings[pos, 2 * i + 1] = torch.cos(\n",
    "                    pos / torch.tensor(10000).pow(2 * i / d_model)\n",
    "                )\n",
    "    assert counter == L * d_model\n",
    "    return encodings\n",
    "\n",
    "\n",
    "PositionalEncoding(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8492/2721896283.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  torch.range(0, 10).reshape(-1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.range(0, 10).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Embedding(nn.Module):\n",
    "#     def __init__(self, vocab_size: int, d_model: int, *args, **kwargs) -> None:\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.embedding = torch.normal(\n",
    "#             mean=0.0, std=0.1, size=(vocab_size, d_model), requires_grad=True\n",
    "#         )\n",
    "\n",
    "#     # TODO: make work in batches\n",
    "#     def forward(self, x_one_hot: torch.Tensor):\n",
    "#         # print(x_one_hot.shape)\n",
    "#         batched_range = torch.arange(self.vocab_size).type(torch.float32)\n",
    "#         batched_range = batched_range.unsqueeze(0).repeat(x_one_hot.shape[1], 1)\n",
    "#         # print(batched_range.shape)\n",
    "#         positions = batched_matmul(x_one_hot, batched_range.transpose(1, 0)).type(torch.int64)\n",
    "\n",
    "#         print(positions)\n",
    "#         print(self.embedding.shape)\n",
    "#         # print(self.embedding)\n",
    "#         return self.embedding[positions]\n",
    "\n",
    "\n",
    "# emb = Embedding(3, 2)\n",
    "# emb.forward(\n",
    "#     torch.FloatTensor(\n",
    "#         [\n",
    "#             [[0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1]],\n",
    "#             [[1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0]],\n",
    "#         ]\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1057, -0.1647],\n",
       "         [ 0.1355, -0.0796],\n",
       "         [ 0.0850,  0.0329],\n",
       "         [ 0.1355, -0.0796]],\n",
       "\n",
       "        [[ 0.0850,  0.0329],\n",
       "         [ 0.1057, -0.1647],\n",
       "         [ 0.0850,  0.0329],\n",
       "         [ 0.1057, -0.1647]]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = torch.normal(\n",
    "            mean=0.0, std=0.1, size=(vocab_size, d_model), requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x_one_hot: torch.Tensor):\n",
    "        positions = torch.matmul(\n",
    "            x_one_hot, torch.arange(self.vocab_size, dtype=torch.float32)\n",
    "        ).type(torch.int64)\n",
    "\n",
    "        # print(positions)\n",
    "        # print(self.embedding)\n",
    "        return self.embedding[positions]\n",
    "\n",
    "\n",
    "# emb = Embedding(vocab_size=3, d_model=2)\n",
    "# emb.forward(\n",
    "#     torch.FloatTensor(\n",
    "#         [\n",
    "#             [[0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1]],\n",
    "#             [[1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0]],\n",
    "#         ]\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        T=T,\n",
    "        d_K=D_K,\n",
    "        d_V=D_V,\n",
    "        d_model=D_MODEL,\n",
    "        h=H,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        n_classes=N_CLASSES,\n",
    "        dropout=0.1,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.T = T\n",
    "        self.d_K = d_K\n",
    "\n",
    "        self.embbeding = Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(L, d_model)\n",
    "\n",
    "        self.transformersBlocks = nn.ModuleList()\n",
    "        for _ in range(N_MHA_BLOCKS_ENCODER):\n",
    "            self.transformersBlocks.append(\n",
    "                TransformerBlock(T=T, d_K=d_K, d_V=d_V, d_model=d_model, h=h)\n",
    "            )\n",
    "\n",
    "        self.prediction_head = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.position_encoding + self.embbeding(x)\n",
    "        x = self.transformersBlocks(x)\n",
    "        x = F.softmax(self.prediction_head(x), dim=-1)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
