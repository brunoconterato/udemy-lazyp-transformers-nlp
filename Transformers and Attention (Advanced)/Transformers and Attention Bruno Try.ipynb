{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Let's build the transformer's encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0. Init\n",
    "\n",
    "* Initialize the notebook environment.\n",
    "* Hyperparameters definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import unittest\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c512930015408596bbd4d114969c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Hello World', description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_w = widgets.Text(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False   \n",
    ")\n",
    "\n",
    "display(test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = test_w.value\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters Here\n",
    "BATCH_SIZE = 128\n",
    "T = 80  # sentence length\n",
    "D_K = 16\n",
    "D_V = 16\n",
    "D_MODEL = 128\n",
    "H = 8\n",
    "VOCAB_SIZE = 10\n",
    "N_TRANSFORMERS_BLOCKS_ENCODER = 6\n",
    "N_CLASSES = 2  # classes of classifier\n",
    "\n",
    "USE_MASK = False\n",
    "MASK_BEFORE_SOFTMAX = True  # Original paper use mask before softmax\n",
    "\n",
    "LOG_INTERVAL_IN_BATCHES = 10\n",
    "EPOCHS = 12\n",
    "\n",
    "ALLOW_GPU = True\n",
    "\n",
    "# Set seeds to reprodutivity\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "RUN_UNIT_TESTS = True\n",
    "TRAIN_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() and ALLOW_GPU else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "# Test softmax x axis:\n",
    "matrix = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "softmaxed_matrix = F.softmax(matrix, dim=1)\n",
    "\n",
    "print(softmaxed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(x: torch.Tensor, n: int):\n",
    "    # make shape (n, 1, 1, ...) --> quantity of 1's must be len(x.shape)\n",
    "    # for example, if shape of x is (3, 4, 8), shapee must be (n, 1, 1, 1)\n",
    "    tuple_ones = tuple(\n",
    "        (torch.tensor(x.shape) / torch.tensor(x.shape)).numpy().astype(int)\n",
    "    )\n",
    "    return x.unsqueeze(0).repeat((n, *tuple_ones))\n",
    "\n",
    "\n",
    "def batched_matmul(tensor_3d, tensor_2d):\n",
    "    W_repeated = repeat(tensor_2d, n=tensor_3d.shape[0])\n",
    "\n",
    "    return torch.bmm(tensor_3d, W_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Convention from: https://www.udemy.com/course/data-science-transformers-nlp/learn/lecture/32255056#overview\n",
    "    In our convention, K, Q and V are learneable, different from the \"Attention is all you need\" paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_K, d_V, d_model: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Define trainable parameters with requires_grad=True\n",
    "        # torch 2d tensor initialized normally\n",
    "        self.W_K = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_K)))\n",
    "        self.W_Q = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_K)))\n",
    "        self.W_V = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_V)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask=None) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the Attention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch, T, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch, T, d_V)\n",
    "            torch.Tensor: Attention weights of shape (batch, T, T)\n",
    "        \"\"\"\n",
    "        # Shapes:\n",
    "        # x is a 3d tensor (batch, T, d_model)\n",
    "        # W_K (d_model, d_K)\n",
    "\n",
    "        # (batch, T, d_model) x (d_model, d_K) -> (batch, T, d_K)\n",
    "        K = batched_matmul(x, self.W_K)\n",
    "        Q = batched_matmul(x, self.W_Q)\n",
    "        V = batched_matmul(x, self.W_V)\n",
    "\n",
    "        # (batch, T, d_K) x (batch, d_k, T) -> (batch, T, T)\n",
    "        result = torch.bmm(Q, K.transpose(1, 2)) / (K.shape[-1] ** 0.5)\n",
    "        \n",
    "        if MASK_BEFORE_SOFTMAX and attention_mask:\n",
    "            result = result.masked_fill(attention_mask == 0, -1e9)  # Set masked elements to -inf\n",
    "        \n",
    "        attention_weights = F.softmax(result, dim=-1)\n",
    "        \n",
    "        if not MASK_BEFORE_SOFTMAX and attention_mask:\n",
    "            attention_weights = attention_weights.masked_fill(attention_mask == 0, 0)\n",
    "        \n",
    "        # (batch, T, T) x (batch, T, d_V) -> (batch, T, d_V)\n",
    "        result = torch.bmm(attention_weights, V)\n",
    "        return result, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16])\n",
      "torch.Size([128, 16])\n",
      "torch.Size([128, 16])\n",
      "torch.Size([128, 80, 16])\n"
     ]
    }
   ],
   "source": [
    "att = Attention(d_K=D_K, d_V=D_V, d_model=D_MODEL)\n",
    "x = torch.normal(mean=0, std=0.01, size=(BATCH_SIZE, T, D_MODEL))\n",
    "\n",
    "att_result, att_weights = att.forward(x)\n",
    "assert att_result.shape == (BATCH_SIZE, T, D_V)\n",
    "print(att.W_K.shape)\n",
    "print(att.W_Q.shape)\n",
    "print(att.W_V.shape)\n",
    "print(att_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, d_K, d_V):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [Attention(d_K=d_K, d_V=d_V, d_model=d_model) for _ in range(h)]\n",
    "        )\n",
    "        self.W_O = nn.Parameter(torch.normal(mean=0, std=0.01, size=(h * d_V, d_model)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"Forward pass of the MultiHeadAttention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch, T, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch, T, d_model)\n",
    "            torch.Tensor: Attention weights of shape (batch, h, T, T)\n",
    "        \"\"\"\n",
    "        attention_results = [attention(x)[0] for attention in self.attentions]\n",
    "        attention_weights = torch.stack(\n",
    "            [attention(x)[1] for attention in self.attentions]\n",
    "        )\n",
    "        concatenated = torch.cat(attention_results, dim=-1)\n",
    "        return batched_matmul(concatenated, self.W_O), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(h=H, d_K=D_K, d_V=D_V, d_model=D_MODEL)\n",
    "out, att_weights = mha(x)\n",
    "assert out.shape == (BATCH_SIZE, T, D_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. The transformer block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (1): Dropout(p=0.1, inplace=False)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (4): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_K=D_K, d_V=D_V, d_model=D_MODEL, h=H, dropout=0.1, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mha = MultiHeadAttention(h, d_K=d_K, d_V=d_V, d_model=d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.layer_norm(x + self.mha(x)[0])\n",
    "        x = self.layer_norm(x + self.ann(x)[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "transformerBlock = TransformerBlock()\n",
    "transformerBlock(x).shape\n",
    "transformerBlock.ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. The positional encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PositionalEncoding(t: int, d_model) -> torch.Tensor:\n",
    "    encodings = torch.zeros(size=(t, d_model), requires_grad=False)\n",
    "    counter = 0\n",
    "    for pos in range(t):\n",
    "        for i in range((d_model // 2) + 1):\n",
    "            if 2 * i < d_model:\n",
    "                counter += 1\n",
    "                encodings[pos, 2 * i] = torch.sin(\n",
    "                    pos / torch.tensor(10000).pow(2 * i / d_model)\n",
    "                )\n",
    "            if 2 * i + 1 < d_model:\n",
    "                counter += 1\n",
    "                encodings[pos, 2 * i + 1] = torch.cos(\n",
    "                    pos / torch.tensor(10000).pow(2 * i / d_model)\n",
    "                )\n",
    "    assert counter == t * d_model\n",
    "    return encodings\n",
    "\n",
    "\n",
    "PositionalEncoding(T, D_MODEL).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38243/2721896283.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  torch.range(0, 10).reshape(-1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.range(0, 10).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. The embedding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding tensor([[ 0.0699, -0.0838],\n",
      "        [ 0.1141, -0.0243],\n",
      "        [-0.0473,  0.1783]])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2, 2])\n",
      "tensor([[[-0.0473,  0.1783],\n",
      "         [ 0.1141, -0.0243]],\n",
      "\n",
      "        [[ 0.1141, -0.0243],\n",
      "         [ 0.0699, -0.0838]],\n",
      "\n",
      "        [[ 0.0699, -0.0838],\n",
      "         [-0.0473,  0.1783]]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: make this work in batches\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, padding_idx=0):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Parameter(\n",
    "            torch.normal(\n",
    "                mean=0.0, std=0.1, size=(vocab_size, d_model), requires_grad=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass Embedding layer\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input Tensor of shape (batch_size, T)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output Tensor of shape (batch_size, T, d_model)\n",
    "        \"\"\"\n",
    "        return self.embedding[x.long()]\n",
    "\n",
    "\n",
    "emb = Embedding(vocab_size=3, d_model=2)\n",
    "# emb = nn.Embedding(num_embeddings=3, embedding_dim=2)\n",
    "for name, param in emb.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "input_emb = torch.FloatTensor([[2, 1], [1, 0], [0, 2]])\n",
    "\n",
    "print(input_emb.shape)\n",
    "\n",
    "out_emb = emb(input_emb.long())\n",
    "print(out_emb.shape)\n",
    "print(out_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        t=T,\n",
    "        d_K=D_K,\n",
    "        d_V=D_V,\n",
    "        d_model=D_MODEL,\n",
    "        h=H,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        n_transformers=N_TRANSFORMERS_BLOCKS_ENCODER,\n",
    "        dropout=0.1,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_K = d_K\n",
    "        self.d_V = d_V\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.position_encoding: torch.Tensor = PositionalEncoding(t, d_model)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "        for _ in range(n_transformers):\n",
    "            self.transformer_blocks.append(\n",
    "                TransformerBlock(d_K=d_K, d_V=d_V, d_model=d_model, h=h)\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batchedPositionalEncoding = self.position_encoding.repeat(x.shape[0], 1, 1)\n",
    "        x = self.embedding(x.int()).float()\n",
    "        x = x + batchedPositionalEncoding\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        self.position_encoding = self.position_encoding.to(*args, **kwargs)\n",
    "        return super().to(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. The Classification Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierEncoder(Encoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes,\n",
    "        t=T,\n",
    "        d_K=D_K,\n",
    "        d_V=D_V,\n",
    "        d_model=D_MODEL,\n",
    "        h=H,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        n_transformers=N_TRANSFORMERS_BLOCKS_ENCODER,\n",
    "        dropout=0.1,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ClassifierEncoder, self).__init__()\n",
    "        self.d_K = d_K\n",
    "        self.d_V = d_V\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.position_encoding: torch.Tensor = PositionalEncoding(t, d_model)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "        for _ in range(n_transformers):\n",
    "            self.transformer_blocks.append(\n",
    "                TransformerBlock(d_K=d_K, d_V=d_V, d_model=d_model, h=h)\n",
    "            )\n",
    "\n",
    "        self.prediction_head = nn.Linear(self.d_model, n_classes)\n",
    "        self.n_classes = n_classes\n",
    "        self.prediction_head = nn.Linear(self.d_model, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = super().forward(x)\n",
    "        x = self.prediction_head(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Let's try a classification problem using the CLassification Encoder\n",
    "* See the file: Fine-Tuning (Intermediate)/Fine-Tunning Sentiment Custom Dataset + Labels.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, lets make a dataset with cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.read_csv(\"../Fine-Tuning (Intermediate)/AirlineTweets.csv\")\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_['airline_sentiment'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_[[\"airline_sentiment\", \"text\"]].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text  target\n",
       "0           neutral                @VirginAmerica What @dhepburn said.       2\n",
       "1          positive  @VirginAmerica plus you've added commercials t...       1\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...       2\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...       0\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...       0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_map = {\n",
    "    \"positive\": 1,\n",
    "    \"negative\": 0,\n",
    "    \"neutral\": 2,\n",
    "}\n",
    "inverse_target_map = {v: k for k, v in target_map.items()}\n",
    "df[\"target\"] = df[\"airline_sentiment\"].map(target_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text  target\n",
       "1          positive  @VirginAmerica plus you've added commercials t...       1\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...       0\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...       0\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...       0\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX...       1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df[df[\"target\"] != 2]\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence,label\n",
      "@VirginAmerica plus you've added commercials to the experience... tacky.,1\n",
      "\"@VirginAmerica it's really aggressive to blast obnoxious \"\"entertainment\"\" in your guests' faces &amp; they have little recourse\",0\n",
      "@VirginAmerica and it's a really big bad thing about it,0\n",
      "\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\n",
      "it's really the only bad thing about flying VA\",0\n",
      "\"@VirginAmerica yes, nearly every time I fly VX this ear worm wont go away :)\",1\n",
      "\"@virginamerica Well, I didn'tbut NOW I DO! :-D\",1\n",
      "\"@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.\",1\n",
      "@VirginAmerica I &lt;3 pretty graphics. so much better than minimal iconography. :D,1\n"
     ]
    }
   ],
   "source": [
    "df2 = df_filtered[['text', 'target']]\n",
    "# Not documented info: targets must have the column name label\n",
    "# sentence may have other names, but not label\n",
    "df2.columns = ['sentence', 'label']\n",
    "df2.to_csv(\"data.csv\", index=False)\n",
    "!head data.csv\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe0bc51938441c6886f7ff116697513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fc48ca2a074d949a07dd968fe58761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c154004ca64ff2b2d1d29c418938b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"csv\", data_files=\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 11541\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainTestValidation(dataset: Dataset, valid_size=0.1, test_size=0.1):\n",
    "    len_valid = int(len(dataset) * valid_size)\n",
    "    len_test = int(len(dataset) * test_size)\n",
    "\n",
    "    splited: DatasetDict = dataset.train_test_split(\n",
    "        len_valid + len_test, shuffle=False, seed=42\n",
    "    )\n",
    "    splited[\"validation\"] = splited[\"test\"]\n",
    "    del splited[\"test\"]\n",
    "\n",
    "    splited_2 = splited[\"validation\"].train_test_split(len_test, shuffle=True, seed=42)\n",
    "    splited[\"validation\"] = splited_2[\"train\"]\n",
    "    splited[\"test\"] = splited_2[\"test\"]\n",
    "\n",
    "    return splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = raw_dataset['train'].train_test_split(test_size=.3, seed=42)\n",
    "split = splitTrainTestValidation(raw_dataset[\"train\"], valid_size=0.1, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 9233\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 1154\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 1154\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=T\n",
    "    )\n",
    "\n",
    "\n",
    "# tokenizer(\"This is an example\", truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd1035ddc764d61bf393acd9d0898f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9233 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99418f1c7ef0454c867e193c494b5420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913decd20bae4356b28f5c4729b5fdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 9233\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1154\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1154\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = split.map(tokenize_fn, batched=False)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df50ccbe2ba24b6cbc4112250d43e607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbed290de694508b86836f557a98c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 9233\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 853\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 844\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Little notation here:\n",
    "# token is an int from the tokenizer\n",
    "# idx is our index, to use in our embedding\n",
    "\n",
    "token2idx = {0: 0}\n",
    "idx2token = {}\n",
    "\n",
    "all_tokens = [\n",
    "    element\n",
    "    for list_ids in tokenized_datasets[\"train\"][\"input_ids\"]\n",
    "    for element in list_ids\n",
    "]\n",
    "all_tokens = list(set(all_tokens))\n",
    "\n",
    "token_index = 0\n",
    "for token in all_tokens:\n",
    "    if token not in token2idx:\n",
    "        token2idx[token] = token_index\n",
    "        idx2token[token_index] = token\n",
    "        token_index += 1\n",
    "\n",
    "\n",
    "def filterSplit(splited_dataset):\n",
    "    \"\"\"For valid and test datasets, get only those which all inpu_ids is in splited_dataset['train']\"\"\"\n",
    "\n",
    "    for split in [\"validation\", \"test\"]:\n",
    "        # Filter the splited_dataset[split] to only keep the ids which are in splited_dataset['train']\n",
    "        splited_dataset[split] = splited_dataset[split].filter(\n",
    "            lambda x: all(token in token2idx for token in x[\"input_ids\"])\n",
    "        )\n",
    "\n",
    "    return splited_dataset\n",
    "\n",
    "\n",
    "filtered_datasets = filterSplit(tokenized_datasets)\n",
    "filtered_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos construir nosso dicionrio de tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66a64dacf484617969e84304f3de88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9233 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4710dc1e08c142ad9dc127ffbf2c92a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f145a6440747e8a4172d29704e1424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/844 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask', 'input_idx'],\n",
       "        num_rows: 9233\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask', 'input_idx'],\n",
       "        num_rows: 853\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'input_ids', 'attention_mask', 'input_idx'],\n",
       "        num_rows: 844\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makeIndex(batch):\n",
    "    batch[\"input_idx\"] = [token2idx[token_id] for token_id in batch[\"input_ids\"]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "data = filtered_datasets.map(makeIndex, batched=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_labels = {\n",
    "    \"Train\": data[\"train\"][\"label\"],\n",
    "    \"Validation\": data[\"validation\"][\"label\"],\n",
    "    \"Test\": data[\"test\"][\"label\"],\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(13, 4))\n",
    "\n",
    "for i, (label_name, labels) in enumerate(data_labels.items()):\n",
    "    axs[i].hist(labels, alpha=0.5, label=label_name)\n",
    "    axs[i].set_xlabel(\"Label\")\n",
    "    axs[i].set_ylabel(\"Count\")\n",
    "    axs[i].set_title(f\"Histogram of {label_name} Labels\")\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.subplots_adjust(wspace=8)  # Adjust the space between the subplots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assert all same length\n",
    "list(set([len(l_idx) for l_idx in data[\"train\"][\"input_idx\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9233, 80])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train = torch.Tensor(data[\"train\"][\"input_idx\"]).type(torch.int32)\n",
    "mask_train = torch.Tensor(data[\"train\"][\"attention_mask\"]).type(torch.int32)\n",
    "input_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train = torch.Tensor(data[\"train\"][\"label\"])\n",
    "\n",
    "# make one_hot\n",
    "target_train = F.one_hot(target_train.to(torch.int64), num_classes=N_CLASSES).float()\n",
    "\n",
    "target_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([853, 80])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_valid = torch.Tensor(data[\"validation\"][\"input_idx\"]).type(torch.int32)\n",
    "mask_valid = torch.Tensor(data[\"validation\"][\"attention_mask\"]).type(torch.int32)\n",
    "input_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_valid = torch.Tensor(data[\"validation\"][\"label\"])\n",
    "\n",
    "# make one_hot\n",
    "target_valid = F.one_hot(target_valid.to(torch.int64), num_classes=N_CLASSES).float()\n",
    "\n",
    "target_valid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([844, 80])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = torch.Tensor(data[\"test\"][\"input_idx\"]).type(torch.int32)\n",
    "mask_test = torch.Tensor(data[\"test\"][\"attention_mask\"]).type(torch.int32)\n",
    "input_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test = torch.Tensor(data[\"test\"][\"label\"])\n",
    "\n",
    "# make one_hot\n",
    "target_test = F.one_hot(target_test.to(torch.int64), num_classes=N_CLASSES).float()\n",
    "\n",
    "target_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's train our model! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_loader, criterion, verbose=False, type_loss=\"validation\"):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs_, target, mask in eval_loader:\n",
    "            inputs_, target = inputs_.to(device), target.to(device)\n",
    "            if USE_MASK:\n",
    "                output = model(inputs_, mask)\n",
    "            else:\n",
    "                output = model(inputs_)\n",
    "                \n",
    "            if type(criterion) == torch.nn.modules.loss.NLLLoss:\n",
    "                loss += criterion(output, target.argmax(dim=-1)).item()\n",
    "            else:\n",
    "                loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            target_ = target.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target_).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    loss /= len(eval_loader.dataset)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    verbose and print(\n",
    "        \"\\nAverage loss ({}): {:.6f}, Accuracy: {}/{} ({:.2f}%)\\n\".format(\n",
    "            type_loss,\n",
    "            loss,\n",
    "            correct,\n",
    "            total,\n",
    "            accuracy,\n",
    "        )\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    total_loss = 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for batch_idx, (input_, target, mask) in enumerate(train_loader):\n",
    "        input_, target = input_.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if USE_MASK:\n",
    "            output = model(input_, mask)\n",
    "        else:\n",
    "            output = model(input_)\n",
    "        if type(criterion) == torch.nn.modules.loss.NLLLoss:\n",
    "            loss = criterion(output, target.argmax(dim=-1))\n",
    "        else:\n",
    "            loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (batch_idx + 1) / len(input_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # valid_loss = evaluate(model, validation_loader, criterion)\n",
    "        # if valid_loss < best_loss:\n",
    "        #     best_loss = valid_loss\n",
    "        #     torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "        if (batch_idx + 1) % LOG_INTERVAL_IN_BATCHES == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain loss (avg): {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(input_),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    avg_loss,\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, target, attention_mask=None):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "        if attention_mask is not None:\n",
    "            self.attention_mask = attention_mask\n",
    "        else:\n",
    "            self.attention_mask = torch.ones_like(inputs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.target[idx], self.attention_mask[idx]\n",
    "\n",
    "\n",
    "model = ClassifierEncoder(\n",
    "    n_classes=N_CLASSES,\n",
    "    t=T,\n",
    "    d_K=D_K,\n",
    "    d_V=D_V,\n",
    "    d_model=D_MODEL,\n",
    "    h=H,\n",
    "    vocab_size=len(token2idx),\n",
    "    n_transformers=N_TRANSFORMERS_BLOCKS_ENCODER,\n",
    ")\n",
    "\n",
    "train_dataset = MyDataset(input_train, target_train, attention_mask=mask_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "valid_dataset = MyDataset(input_valid, target_valid, attention_mask=mask_valid)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = MyDataset(input_test, target_test, attention_mask=mask_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# criterion = torch.nn.NLLLoss()\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "best_loss = np.inf\n",
    "valid_losses = []\n",
    "\n",
    "\n",
    "def plotLosses(losses, loss_type=\"validation\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title(f\"Model loss ({loss_type})\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    valid_losses = []\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train(model, train_loader, optimizer, criterion, epoch)\n",
    "        valid_loss = evaluate(\n",
    "            model, valid_loader, criterion, verbose=True, type_loss=\"validation\"\n",
    "        )\n",
    "        valid_losses.append(valid_loss)\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"model.pth\")\n",
    "        print(f\"Epoch: {epoch}\\tloss (validation): {valid_loss}\")\n",
    "\n",
    "\n",
    "# If model is saved, load it\n",
    "if os.path.exists(\"model.pth\"):\n",
    "    model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    test_loss = evaluate(\n",
    "        model, test_loader, criterion, verbose=True, type_loss=\"test\"\n",
    "    )\n",
    "    plotLosses(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: @AmericanAir \n",
      "It's not what happens to us that matters...It's our response that matters. Way to drop the ball AA.\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.9825472235679626\n",
      "\n",
      "Sentence: @AmericanAir Can't unload flight #3322 because jetway is broken.  #steps #planB? #waiting nearly an hour\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.9971217513084412\n",
      "\n",
      "Sentence: @AmericanAir If the flight I selected online was what was ticketed I would not be missing my connection. I need help getting to DFW or IAH!!\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.9784184098243713\n",
      "\n",
      "Sentence: @AmericanAir I've been calling you for 3 straight days and no one picks up. Sure there are storms but there are also #customers Holler!\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.99539715051651\n",
      "\n",
      "Sentence: @AmericanAir No. I was told I got put on another flight and that I would get an email. Still haven't gotten one yet.\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.9883008599281311\n",
      "\n",
      "Sentence: @AmericanAir i was spoken 2 like I'm an idiot and that is not OK!! I don't need to deal w/ that esp after the travel experience I've had\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.9831499457359314\n",
      "\n",
      "Sentence: @AmericanAir now a delay to the rebooked flight?!\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.981971800327301\n",
      "\n",
      "Sentence: @AmericanAir How are you going to compensate all of whose days/plans have been ruined because of the AA3490 delay/pending Cancelled Flightlation?\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.9978873133659363\n",
      "\n",
      "Sentence: @AmericanAir what are my chances of making a connection to El Paso (AA504) with DFW from SAT (AA200) delayed 30 minutes?\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.9902432560920715\n",
      "\n",
      "Sentence: @AmericanAir they don't even give an option to hold.. Just say lines are busy Plz try Late Flightr\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.994724690914154\n",
      "\n",
      "Sentence: @AmericanAir there's an employee at gate 45 of JFK telling people their bag doesn't fit and needs to be checked when it clearly fits #AA291\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.9928914308547974\n",
      "\n",
      "Sentence: @AmericanAir I love very much your planes, can you please follow me back? It's an amazing bussines!\n",
      "True label: positive\n",
      "Prediction: positive   Proba: 0.6123889684677124\n",
      "\n",
      "Sentence: @AmericanAir Thanks for asking On second plane after maintenance issue, for flight from ORD to LIT. Sitting at gate in very very warm plane\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.7851359844207764\n",
      "\n",
      "Sentence: @AmericanAir She was at gate on time though, they made her cry, and made a scene for 10 mins instead of boarding her. Plane still outside.\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.9482794404029846\n",
      "\n",
      "Sentence: @AmericanAir-everyone: its been weeks&amp;those dickheads @ AA hadn't contacted me...they lost my checked bag &amp; carryon and haven't offer to pay\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.9216505289077759\n",
      "\n",
      "Sentence: @AmericanAir Oh, and losing my luggage #ridiculous # angrybird # where'smybag\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.8719593286514282\n",
      "\n",
      "Sentence: @AmericanAir @usairways who is the next stop after customer service.\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.9587869048118591\n",
      "\n",
      "Sentence: @AmericanAir ok makes no sense tho Since you'll give me a free upgrade to first.\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.831616997718811\n",
      "\n",
      "Sentence: @AmericanAir I was rebooked on a flight that was too Late Flight for my connection!\n",
      "True label: negative\n",
      "Prediction: negative   Proba: 0.8573622703552246\n",
      "\n",
      "Sentence: @AmericanAir @dfwairport Guys, let it go. http://t.co/vOxcghciJi\n",
      "True label: positive\n",
      "Prediction: positive   Proba: 0.5522142052650452\n"
     ]
    }
   ],
   "source": [
    "# Let's print the predictions of the model\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    for sentence, true_label in zip(data[\"test\"][\"sentence\"][100:130], data[\"test\"][\"label\"][60:80]):\n",
    "        tokenized_ = tokenizer(\n",
    "            sentence, truncation=True, padding=\"max_length\", max_length=T\n",
    "        )\n",
    "        input_ = torch.tensor(\n",
    "            [token2idx[token_id] for token_id in tokenized_[\"input_ids\"]]\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        mask = torch.tensor(tokenized_[\"attention_mask\"])\n",
    "        input_, mask = input_.to(device), mask.to(device)\n",
    "        if USE_MASK:\n",
    "            output = model(input_, mask)\n",
    "        else:\n",
    "            output = model(input_)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        print(\n",
    "            f\"\"\"\\nSentence: {sentence}\n",
    "True label: {inverse_target_map[pred.item()]}\n",
    "Prediction: {inverse_target_map[pred.item()]}   Proba: {output[0][pred.item()].item()}\"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit tests for transformer and attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTensorFunctions(unittest.TestCase):\n",
    "    def test_repeat(self):\n",
    "        x = torch.tensor([1, 2, 3])\n",
    "        n = 2\n",
    "        result = repeat(x, n)\n",
    "        expected = torch.tensor([[1, 2, 3], [1, 2, 3]])\n",
    "        self.assertTrue(torch.equal(result, expected))\n",
    "\n",
    "    def test_batched_matmul(self):\n",
    "        tensor_3d = torch.randn(10, 3, 4)\n",
    "        tensor_2d = torch.randn(4, 5)\n",
    "        result = batched_matmul(tensor_3d, tensor_2d)\n",
    "        expected = torch.bmm(tensor_3d, tensor_2d.unsqueeze(0).repeat((10, 1, 1)))\n",
    "        self.assertTrue(torch.allclose(result, expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayerTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Set up your embedding layer or load your model\n",
    "        self.vocab_size = 10\n",
    "        self.d_model = 8\n",
    "        self.batch_size = 2\n",
    "        self.t = 4\n",
    "        self.model = Embedding(self.vocab_size, self.d_model)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def test_embedding_layer_forward(self):\n",
    "        # Define input tensor (batch_size=2, sequence_length=4)\n",
    "        x = torch.tensor([[1, 3, 5, 7], [0, 2, 4, 6]])\n",
    "\n",
    "        # Manually set model weights for the embedding matrix\n",
    "        self.model.embedding.data = torch.tensor(\n",
    "            [\n",
    "                [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "                [0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6],\n",
    "                [1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4],\n",
    "                [2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2],\n",
    "                [3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0],\n",
    "                [4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8],\n",
    "                [4.9, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6],\n",
    "                [5.7, 5.8, 5.9, 6.0, 6.1, 6.2, 6.3, 6.4],\n",
    "                [6.5, 6.6, 6.7, 6.8, 6.9, 7.0, 7.1, 7.2],\n",
    "                [7.3, 7.4, 7.5, 7.6, 7.7, 7.8, 7.9, 8.0],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Calculate the expected output manually\n",
    "        expected_output = torch.tensor(\n",
    "            [\n",
    "                [\n",
    "                    [\n",
    "                        [0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6],\n",
    "                        [2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2],\n",
    "                        [4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8],\n",
    "                        [5.7, 5.8, 5.9, 6.0, 6.1, 6.2, 6.3, 6.4],\n",
    "                    ],\n",
    "                    [\n",
    "                        [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "                        [1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4],\n",
    "                        [3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0],\n",
    "                        [4.9, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6],\n",
    "                    ],\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Check if the shape of the expected output matches\n",
    "        self.assertEqual(expected_output.shape, expected_output.shape)\n",
    "\n",
    "        # Check if the values of the expected output tensor match\n",
    "        self.assertTrue(torch.allclose(expected_output, expected_output, atol=1e-6))\n",
    "\n",
    "    def test_embedding_layer_loss(self):\n",
    "        # Define input tensors and target tensors\n",
    "        input_tensor = torch.randint(\n",
    "            0, self.vocab_size, (self.batch_size, self.t)\n",
    "        ).long()\n",
    "        target_tensor = torch.randn(self.batch_size, self.t, self.d_model)\n",
    "\n",
    "        # Forward pass through the embedding layer\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.criterion(output, target_tensor)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param.grad != 0), \"No gradients in the parameters\"\n",
    "            )\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAttention(unittest.TestCase):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        unittest (_type_): unit test for a conjuntion linear layer\n",
    "            plus the attention block\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class AttentionTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 2\n",
    "        self.d_k = 3\n",
    "        self.d_v = 3\n",
    "        self.T = 2\n",
    "        self.d_model = 4\n",
    "\n",
    "        # Set up your attention mechanism or load your model\n",
    "        self.model = Attention(self.d_k, self.d_v, self.d_model)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def test_attention_forward(self):\n",
    "        # Define input tensor (batch_size=2, sequence_length=2, d_model=4)\n",
    "        x = torch.tensor(\n",
    "            [\n",
    "                [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]],\n",
    "                [[9.0, 10.0, 11.0, 12.0], [13.0, 14.0, 15.0, 16.0]],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Manually set model weights for W_K, W_Q, and W_V\n",
    "        self.model.W_K.data = torch.tensor(\n",
    "            [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.model.W_Q.data = torch.tensor(\n",
    "            [[1.2, 1.1, 1.0], [0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.model.W_V.data = torch.tensor(\n",
    "            [[0.5, 0.6, 0.7], [0.8, 0.9, 1.0], [1.1, 1.2, 1.3], [1.4, 1.5, 1.6]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "\n",
    "        # Expected output based on calculations from the attention mechanism\n",
    "        expected_output, _ = self.model(x)\n",
    "\n",
    "        # Calculate the expected output manually\n",
    "        # Calculate Q, K, and V using learned weights\n",
    "        Q = torch.matmul(x, self.model.W_Q)\n",
    "        K = torch.matmul(x, self.model.W_K)\n",
    "        V = torch.matmul(x, self.model.W_V)\n",
    "\n",
    "        # Calculate the attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(1, 2)) / (self.d_k**0.5)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Apply attention weights to V to get the output\n",
    "        expected_output_manual = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Check if the shape of the expected output matches\n",
    "        self.assertEqual(expected_output.shape, expected_output_manual.shape)\n",
    "\n",
    "        # Check if the values of the expected output tensor match\n",
    "        self.assertTrue(\n",
    "            torch.allclose(expected_output, expected_output_manual, atol=1e-6)\n",
    "        )\n",
    "\n",
    "    def test_attention_loss(self):\n",
    "        # Test the loss function associated with the attention mechanism\n",
    "        # Define input tensors and target tensors\n",
    "\n",
    "        input_tensor = torch.rand(\n",
    "            (self.batch_size, self.T, self.d_model), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # Forward pass through the attention mechanism\n",
    "        output, _ = self.model(input_tensor)\n",
    "\n",
    "        # Create dummy target tensor (replace with your actual target tensor)\n",
    "        target = torch.rand_like(output, requires_grad=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.criterion(output, target)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if gradients are computed\n",
    "        self.assertTrue(\n",
    "            input_tensor.grad is not None, \"No gradients in the input tensor\"\n",
    "        )\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[159.9200, 174.0800, 188.2400, 202.4000],\n",
      "         [159.9200, 174.0800, 188.2400, 202.4000]],\n",
      "\n",
      "        [[344.5600, 375.0400, 405.5200, 436.0000],\n",
      "         [344.5600, 375.0400, 405.5200, 436.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# Code by GPT4 to calculate the multihead attention output\n",
    "\n",
    "# Define the parameters\n",
    "d_k = 3\n",
    "d_v = 3\n",
    "d_model = 4\n",
    "h = 2\n",
    "batch_size = 2\n",
    "t = 2\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = torch.tensor(\n",
    "    [\n",
    "        [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]],\n",
    "        [[9.0, 10.0, 11.0, 12.0], [13.0, 14.0, 15.0, 16.0]],\n",
    "    ],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Define the weights\n",
    "W_K = torch.tensor(\n",
    "    [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "W_Q = torch.tensor(\n",
    "    [[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 1.0], [1.1, 1.2, 1.3]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "W_V = torch.tensor(\n",
    "    [[0.3, 0.4, 0.5], [0.6, 0.7, 0.8], [0.9, 1.0, 1.1], [1.2, 1.3, 1.4]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "W_O = torch.tensor(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4],\n",
    "        [0.5, 0.6, 0.7, 0.8],\n",
    "        [0.9, 1.0, 1.1, 1.2],\n",
    "        [1.3, 1.4, 1.5, 1.6],\n",
    "        [1.7, 1.8, 1.9, 2.0],\n",
    "        [2.1, 2.2, 2.3, 2.4],\n",
    "    ],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Initialize tensors for K, Q, V for each head\n",
    "Ks, Qs, Vs = [], [], []\n",
    "\n",
    "# Compute K, Q, V for each head\n",
    "for _ in range(h):\n",
    "    K = torch.matmul(input_tensor, W_K)\n",
    "    Q = torch.matmul(input_tensor, W_Q)\n",
    "    V = torch.matmul(input_tensor, W_V)\n",
    "    Ks.append(K)\n",
    "    Qs.append(Q)\n",
    "    Vs.append(V)\n",
    "\n",
    "# Initialize a tensor for the concatenated results\n",
    "concatenated_results = torch.zeros(batch_size, t, h * d_v)\n",
    "\n",
    "# Calculate the attention and concatenate results for each head\n",
    "for a_idx in range(h):\n",
    "    # Scaled dot product of Q and K\n",
    "    attention_scores = torch.matmul(Qs[a_idx], Ks[a_idx].transpose(-2, -1)) / (\n",
    "        d_k**0.5\n",
    "    )\n",
    "    attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    # Weighted sum of V\n",
    "    head_output = torch.matmul(attention_weights, Vs[a_idx])\n",
    "\n",
    "    # Concatenate results\n",
    "    concatenated_results[:, :, a_idx * d_v : (a_idx + 1) * d_v] = head_output\n",
    "\n",
    "# Apply the final linear layer W_O\n",
    "expected_output = torch.matmul(concatenated_results, W_O)\n",
    "\n",
    "print(expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionTest(unittest.TestCase):\n",
    "    @staticmethod\n",
    "    def calculate_expected_output(input_tensor, batch_size, t, h, d_k, d_v):\n",
    "        # Code by GPT4 to calculate the multihead attention output\n",
    "\n",
    "        # Define the weights\n",
    "        W_K = torch.tensor(\n",
    "            [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        W_Q = torch.tensor(\n",
    "            [[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 1.0], [1.1, 1.2, 1.3]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        W_V = torch.tensor(\n",
    "            [[0.3, 0.4, 0.5], [0.6, 0.7, 0.8], [0.9, 1.0, 1.1], [1.2, 1.3, 1.4]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        W_O = torch.tensor(\n",
    "            [\n",
    "                [0.1, 0.2, 0.3, 0.4],\n",
    "                [0.5, 0.6, 0.7, 0.8],\n",
    "                [0.9, 1.0, 1.1, 1.2],\n",
    "                [1.3, 1.4, 1.5, 1.6],\n",
    "                [1.7, 1.8, 1.9, 2.0],\n",
    "                [2.1, 2.2, 2.3, 2.4],\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "\n",
    "        # Initialize tensors for K, Q, V for each head\n",
    "        Ks, Qs, Vs = [], [], []\n",
    "\n",
    "        # Compute K, Q, V for each head\n",
    "        for _ in range(h):\n",
    "            K = torch.matmul(input_tensor, W_K)\n",
    "            Q = torch.matmul(input_tensor, W_Q)\n",
    "            V = torch.matmul(input_tensor, W_V)\n",
    "            Ks.append(K)\n",
    "            Qs.append(Q)\n",
    "            Vs.append(V)\n",
    "\n",
    "        # Initialize a tensor for the concatenated results\n",
    "        concatenated_results = torch.zeros(batch_size, t, h * d_v)\n",
    "\n",
    "        # Calculate the attention and concatenate results for each head\n",
    "        for a_idx in range(h):\n",
    "            # Scaled dot product of Q and K\n",
    "            attention_scores = torch.matmul(Qs[a_idx], Ks[a_idx].transpose(-2, -1)) / (\n",
    "                d_k**0.5\n",
    "            )\n",
    "            attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "            # Weighted sum of V\n",
    "            head_output = torch.matmul(attention_weights, Vs[a_idx])\n",
    "\n",
    "            # Concatenate results\n",
    "            concatenated_results[:, :, a_idx * d_v : (a_idx + 1) * d_v] = head_output\n",
    "\n",
    "        # Apply the final linear layer W_O\n",
    "        expected_output = torch.matmul(concatenated_results, W_O)\n",
    "\n",
    "        return expected_output\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.h = 2\n",
    "        self.d_k = 3\n",
    "        self.d_v = 3\n",
    "        self.d_model = 4\n",
    "        self.batch_size = 2\n",
    "        self.t = 2\n",
    "        \n",
    "        # Initialize the multi-head attention model\n",
    "        self.model = MultiHeadAttention(h=self.h, d_K=self.d_k, d_V=self.d_v, d_model=self.d_model)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Manually setting the weights for the test\n",
    "        for head in self.model.attentions:\n",
    "            # These weights should be carefully chosen to ensure a predictable output\n",
    "            head.W_K = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]\n",
    "                )\n",
    "            )\n",
    "            head.W_Q = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 1.0], [1.1, 1.2, 1.3]]\n",
    "                )\n",
    "            )\n",
    "            head.W_V = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [[0.3, 0.4, 0.5], [0.6, 0.7, 0.8], [0.9, 1.0, 1.1], [1.2, 1.3, 1.4]]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output linear layer weights\n",
    "        self.model.W_O = nn.Parameter(\n",
    "            torch.tensor(\n",
    "                [\n",
    "                    [0.1, 0.2, 0.3, 0.4],\n",
    "                    [0.5, 0.6, 0.7, 0.8],\n",
    "                    [0.9, 1.0, 1.1, 1.2],\n",
    "                    [1.3, 1.4, 1.5, 1.6],\n",
    "                    [1.7, 1.8, 1.9, 2.0],\n",
    "                    [2.1, 2.2, 2.3, 2.4],\n",
    "                ],\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Shape of W_O, according to the paper \"Attention Is All You Need\"\n",
    "        self.assertEqual(list(self.model.W_O.shape), [self.h * self.d_v, self.d_model])\n",
    "\n",
    "    def test_multi_head_attention_forward(self):\n",
    "        # Define a specific input tensor (batch_size, t, d_model)\n",
    "        input_tensor = torch.tensor(\n",
    "            [\n",
    "                [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]],\n",
    "                [[9.0, 10.0, 11.0, 12.0], [13.0, 14.0, 15.0, 16.0]],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Expected output tensor based on the input and the model weights\n",
    "        # This tensor should be calculated manually based on the model's operations\n",
    "        expected_output = self.calculate_expected_output(\n",
    "            input_tensor, self.batch_size, self.t, self.h, self.d_k, self.d_v\n",
    "        )\n",
    "\n",
    "        # Run the forward pass\n",
    "        output, _ = self.model(input_tensor)\n",
    "\n",
    "        # Check the shape of the output tensor\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.t, self.d_model))\n",
    "\n",
    "        # Check the values of the output tensor\n",
    "        self.assertTrue(torch.allclose(output, expected_output, atol=1e-4))\n",
    "\n",
    "    def test_multi_head_attention_loss(self):\n",
    "        # Test the loss function associated with the multi-head attention mechanism\n",
    "        # Define input tensors and target tensors\n",
    "\n",
    "        input_tensor = torch.rand(\n",
    "            (self.batch_size, self.t, self.d_model), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # Forward pass through the multi-head attention mechanism\n",
    "        output, _ = self.model(input_tensor)\n",
    "\n",
    "        # Create dummy target tensor (replace with your actual target tensor)\n",
    "        target = torch.rand_like(output, requires_grad=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.criterion(output, target)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if gradients are computed\n",
    "        self.assertTrue(\n",
    "            input_tensor.grad is not None, \"No gradients in the input tensor\"\n",
    "        )\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_output(x, attentions, W_O):\n",
    "    batch_size, T, d_model = x.shape\n",
    "    h = len(attentions)\n",
    "    d_k, d_v = attentions[0].W_K.shape[1], attentions[0].W_V.shape[1]\n",
    "\n",
    "    # Initialize the tensor to hold the concatenated outputs from each head\n",
    "    concatenated_heads = torch.zeros((batch_size, T, h * d_v))\n",
    "\n",
    "    for i in range(h):\n",
    "        W_K, W_Q, W_V = attentions[i].W_K, attentions[i].W_Q, attentions[i].W_V\n",
    "\n",
    "        # Calculate K, Q, V matrices\n",
    "        K = x @ W_K\n",
    "        Q = x @ W_Q\n",
    "        V = x @ W_V\n",
    "\n",
    "        # Calculate attention scores and apply softmax (simplified here)\n",
    "        attention_scores = (Q @ K.transpose(-2, -1)) / (d_k**0.5)\n",
    "        # In a real scenario, apply softmax to attention_scores here\n",
    "\n",
    "        # Calculate the weighted sum of V\n",
    "        weighted_sum = attention_scores @ V\n",
    "\n",
    "        # Concatenate results from each head\n",
    "        concatenated_heads[:, :, i * d_v : (i + 1) * d_v] = weighted_sum\n",
    "\n",
    "    # Multiply with W_O\n",
    "    expected_output = concatenated_heads @ W_O\n",
    "    return expected_output\n",
    "\n",
    "\n",
    "class TransformerBlockTest(unittest.TestCase):\n",
    "    def calculate_attention_output(self, x, W_K, W_Q, W_V, d_k):\n",
    "        \"\"\"\n",
    "        Calculate the output of a single head attention.\n",
    "        \"\"\"\n",
    "        K = x @ W_K\n",
    "        Q = x @ W_Q\n",
    "        V = x @ W_V\n",
    "\n",
    "        attention_scores = (Q @ K.transpose(-2, -1)) / (d_k**0.5)\n",
    "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = attention_scores @ V\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "    def calculate_expected_output(\n",
    "        self, x, W_Ks, W_Qs, W_Vs, W_O, linear_weights, linear_biases\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculate the expected output of the TransformerBlock.\n",
    "        \"\"\"\n",
    "        batch_size, T, d_model = x.shape\n",
    "        h = len(W_Ks)\n",
    "        d_k, d_v = W_Ks[0].shape[1], W_Vs[0].shape[1]\n",
    "\n",
    "        # Step 1: Multi-head attention\n",
    "        concatenated_heads = torch.zeros((batch_size, T, h * d_v))\n",
    "        for i in range(h):\n",
    "            attention_output = self.calculate_attention_output(\n",
    "                x, W_Ks[i], W_Qs[i], W_Vs[i], d_k\n",
    "            )\n",
    "            concatenated_heads[:, :, i * d_v : (i + 1) * d_v] = attention_output\n",
    "\n",
    "        multihead_output = concatenated_heads @ W_O\n",
    "\n",
    "        # Step 2: Apply LayerNorm (simplified version)\n",
    "        norm_output = F.layer_norm(multihead_output, multihead_output.shape[1:])\n",
    "\n",
    "        # Step 3: ANN layers (assuming two linear layers with ReLU and Dropout in between)\n",
    "        ann_output = F.linear(norm_output, linear_weights[0], linear_biases[0])\n",
    "        ann_output = F.dropout(ann_output, p=0.1)\n",
    "        ann_output = F.relu(ann_output)\n",
    "        ann_output = F.linear(ann_output, linear_weights[1], linear_biases[1])\n",
    "\n",
    "        return ann_output\n",
    "\n",
    "    def setUp(self):\n",
    "        self.batch_size = 2\n",
    "        self.d_k = 3\n",
    "        self.d_v = 3\n",
    "        self.t = 4\n",
    "        self.d_model = 3\n",
    "        self.h = 2\n",
    "        # Set up your Transformer block model\n",
    "        self.model = TransformerBlock(\n",
    "            d_K=self.d_k, d_V=self.d_v, d_model=self.d_model, h=self.h\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "def test_transformer_block_forward(self):\n",
    "    # Define the input tensor\n",
    "    x = torch.tensor(\n",
    "        [\n",
    "            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]],\n",
    "            [\n",
    "                [13.0, 14.0, 15.0],\n",
    "                [16.0, 17.0, 18.0],\n",
    "                [19.0, 20.0, 21.0],\n",
    "                [22.0, 23.0, 24.0],\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Manually set and collect the weights for each Attention layer\n",
    "    W_Ks = []\n",
    "    W_Qs = []\n",
    "    W_Vs = []\n",
    "    for i, attention in enumerate(self.model.mha.attentions):\n",
    "        W_K = torch.tensor(\n",
    "            [\n",
    "                [0.1 * (i + 1), 0.2 * (i + 1), 0.3 * (i + 1)],\n",
    "                [0.4 * (i + 1), 0.5 * (i + 1), 0.6 * (i + 1)],\n",
    "                [0.7 * (i + 1), 0.8 * (i + 1), 0.9 * (i + 1)],\n",
    "            ]\n",
    "        )\n",
    "        W_Q = torch.tensor(\n",
    "            [\n",
    "                [0.9 * (i + 1), 0.8 * (i + 1), 0.7 * (i + 1)],\n",
    "                [0.6 * (i + 1), 0.5 * (i + 1), 0.4 * (i + 1)],\n",
    "                [0.3 * (i + 1), 0.2 * (i + 1), 0.1 * (i + 1)],\n",
    "            ]\n",
    "        )\n",
    "        W_V = torch.tensor(\n",
    "            [\n",
    "                [0.1 * (i + 1), 0.1 * (i + 1), 0.1 * (i + 1)],\n",
    "                [0.2 * (i + 1), 0.2 * (i + 1), 0.2 * (i + 1)],\n",
    "                [0.3 * (i + 1), 0.3 * (i + 1), 0.3 * (i + 1)],\n",
    "            ]\n",
    "        )\n",
    "        attention.W_K.data = W_K\n",
    "        attention.W_Q.data = W_Q\n",
    "        attention.W_V.data = W_V\n",
    "        W_Ks.append(W_K)\n",
    "        W_Qs.append(W_Q)\n",
    "        W_Vs.append(W_V)\n",
    "\n",
    "    # Set and collect weights for W_O in MultiHeadAttention\n",
    "    W_O = torch.tensor(\n",
    "        [\n",
    "            [0.1, 0.2, 0.3],\n",
    "            [0.4, 0.5, 0.6],\n",
    "            [0.7, 0.8, 0.9],\n",
    "            [0.9, 0.8, 0.7],\n",
    "            [0.6, 0.5, 0.4],\n",
    "            [0.3, 0.2, 0.1],\n",
    "        ]\n",
    "    )\n",
    "    self.model.mha.W_O.data = W_O\n",
    "\n",
    "    # Collect weights and biases for ANN layers (Assuming two linear layers)\n",
    "    linear_weights = [self.model.ann[0].weight, self.model.ann[3].weight]\n",
    "    linear_biases = [self.model.ann[0].bias, self.model.ann[3].bias]\n",
    "\n",
    "    # Calculate the expected output tensor\n",
    "    expected_output = self.calculate_expected_output(\n",
    "        x, W_Ks, W_Qs, W_Vs, W_O, linear_weights, linear_biases\n",
    "    )\n",
    "\n",
    "    # Forward pass\n",
    "    output, _ = self.model(x)\n",
    "\n",
    "    # Check the shape of the output tensor\n",
    "    self.assertEqual(output.shape, expected_output.shape)\n",
    "\n",
    "    # Check the values of the output tensor\n",
    "    self.assertTrue(torch.allclose(output, expected_output, atol=1e-6))\n",
    "\n",
    "    def test_transformer_block_loss(self):\n",
    "        # Test the loss function associated with the Transformer block\n",
    "        # Define input tensors and target tensors\n",
    "\n",
    "        input_tensor = torch.rand(\n",
    "            (self.batch_size, self.t, self.d_model), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # Forward pass through the Transformer block\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # Create dummy target tensor (replace with your actual target tensor)\n",
    "        target = torch.rand_like(output, requires_grad=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.criterion(output, target)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if gradients are computed\n",
    "        self.assertTrue(\n",
    "            input_tensor.grad is not None, \"No gradients in the input tensor\"\n",
    "        )\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2,  ..., 7, 8, 9],\n",
       "        [9, 0, 1,  ..., 6, 7, 8],\n",
       "        [8, 9, 0,  ..., 5, 6, 7],\n",
       "        ...,\n",
       "        [5, 6, 7,  ..., 2, 3, 4],\n",
       "        [4, 5, 6,  ..., 1, 2, 3],\n",
       "        [3, 4, 5,  ..., 0, 1, 2]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTestX(batch_size=BATCH_SIZE, t=T, vocab_size=VOCAB_SIZE):\n",
    "    x = torch.arange(start=0, end=batch_size * t).reshape(batch_size, t) % vocab_size\n",
    "\n",
    "    # each row (wor of index i) of x must be translocated by i positions\n",
    "    for i in range(batch_size):\n",
    "        x[i] = torch.roll(x[i], i)\n",
    "    return x\n",
    "\n",
    "\n",
    "getTestX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    matmul_qk = torch.matmul(Q, K.transpose(-2, -1))  # Scaled QK^T\n",
    "    d_k = Q.size(-1)\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(d_k)\n",
    "    attention_weights = F.softmax(\n",
    "        scaled_attention_logits, dim=-1\n",
    "    )  # Softmax over last axis (seq_len_k)\n",
    "    output = torch.matmul(attention_weights, V)  # Softmax(QK^T)V\n",
    "    return output\n",
    "\n",
    "\n",
    "def calculate_expected_output(x, model):\n",
    "    \"\"\"This function must calculate the expected output of the model, given the input x.\n",
    "    This function must not forward pass the model, but rather calculate the expected output\n",
    "    The expected output is function of the weights of the model, which are defined in the test_forward method\n",
    "    of the ClassifierEncoderTest class.\n",
    "    This function must follow the \"Attention is all you need\" paper.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of shape (batch, T)\n",
    "        model (_type_): _description_\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    t = x.shape[1]\n",
    "\n",
    "    embedded_x = model.embedding(x)\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(embedded_x, model.embedding(x))\n",
    "\n",
    "    def positional_encoding(max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        div_term = div_term.unsqueeze(0)\n",
    "\n",
    "        for i in range(max_len):\n",
    "            for j in range(0, d_model, 2):\n",
    "                pe[i, j] = torch.sin(position[i] * div_term[0, j // 2])\n",
    "                if j + 1 < d_model:\n",
    "                    pe[i, j + 1] = torch.cos(position[i] * div_term[0, j // 2])\n",
    "\n",
    "        return pe.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    position_encoding = positional_encoding(t, model.d_model)\n",
    "    assert position_encoding.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(position_encoding, model.position_encoding)\n",
    "\n",
    "    transformer_input = embedded_x + position_encoding\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(\n",
    "        transformer_input, model.embedding(x) + model.position_encoding\n",
    "    )\n",
    "\n",
    "    # Transformer Blocks\n",
    "    for block in model.transformer_blocks:\n",
    "        # Multi-Head Attention\n",
    "        mha_output = torch.zeros_like(transformer_input)\n",
    "        assert mha_output.shape == (batch_size, t, model.d_model)\n",
    "        head_outputs = torch.zeros((batch_size, t, model.d_V * model.h))\n",
    "        for head_idx, head in enumerate(block.mha.attentions):\n",
    "            Q = batched_matmul(transformer_input, head.W_Q)\n",
    "            K = batched_matmul(transformer_input, head.W_K)\n",
    "            V = batched_matmul(transformer_input, head.W_V)\n",
    "            head_output = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "            # health checking forward: assert model forward equals simulated forward\n",
    "            assert torch.allclose(head_output, head.forward(transformer_input)[0])\n",
    "\n",
    "            # Assert the shape of the output of the head,\n",
    "            # bases on the \"Attention is all you need\" paper\n",
    "            assert head_output.shape == (batch_size, t, model.d_V)\n",
    "\n",
    "            head_outputs[\n",
    "                :, :, model.d_V * head_idx : model.d_V * (head_idx + 1)\n",
    "            ] = head_output\n",
    "\n",
    "        mha_output += batched_matmul(head_outputs, block.mha.W_O)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(mha_output, block.mha(transformer_input)[0])\n",
    "\n",
    "        # Layer Normalization\n",
    "        normed_mha_output = block.layer_norm(\n",
    "            mha_output + transformer_input\n",
    "        )  # Assuming residual connection\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(\n",
    "            normed_mha_output,\n",
    "            block.layer_norm(block.mha(transformer_input)[0] + transformer_input),\n",
    "        )\n",
    "\n",
    "        # Do it now...\n",
    "        # 1. Linear\n",
    "        ann_output = (\n",
    "            torch.matmul(normed_mha_output, block.ann[0].weight.T) + block.ann[0].bias\n",
    "        )\n",
    "        assert ann_output.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(ann_output, block.ann[0](normed_mha_output))\n",
    "\n",
    "        # 2. Dropout\n",
    "        # ann_output = block.ann[1](ann_output)\n",
    "        # assert ann_output.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # 3. ReLU\n",
    "        ann_output_relu = F.relu(ann_output)\n",
    "        assert ann_output_relu.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(ann_output_relu, block.ann[2](ann_output))\n",
    "\n",
    "        # 4. Linear\n",
    "        ann_output_linear_2 = (\n",
    "            torch.matmul(ann_output_relu, block.ann[3].weight.T) + block.ann[3].bias\n",
    "        )\n",
    "        assert ann_output_linear_2.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(ann_output_linear_2, block.ann[3](ann_output_relu))\n",
    "\n",
    "        # 5. Dropout\n",
    "        # ann_output_linear_2 = block.ann[4](ann_output_linear_2)\n",
    "        # assert ann_output_linear_2.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Layer Normalization\n",
    "        normed_ann_output_linear_2 = block.layer_norm(\n",
    "            ann_output_linear_2 + normed_mha_output\n",
    "        )\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(\n",
    "            normed_ann_output_linear_2,\n",
    "            block.layer_norm(ann_output_linear_2 + normed_mha_output),\n",
    "        )\n",
    "\n",
    "        # Prepare for next block\n",
    "        transformer_input = normed_ann_output_linear_2\n",
    "\n",
    "    # Prediction Head\n",
    "\n",
    "    # 1. Linear\n",
    "    output_linear = (\n",
    "        torch.matmul(transformer_input, model.prediction_head.weight.T)\n",
    "        + model.prediction_head.bias\n",
    "    )\n",
    "    assert output_linear.shape == (batch_size, t, model.n_classes)\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(output_linear, model.prediction_head(transformer_input))\n",
    "\n",
    "    # 2. Dropout\n",
    "    # final_output = model.prediction_head[1](final_output)\n",
    "    # assert final_output.shape == (batch_size, t, model.n_classes)\n",
    "\n",
    "    # 3. Softmax\n",
    "    final_output = nn.Softmax(dim=-1)(output_linear)\n",
    "\n",
    "    # return only the last value of the sequence\n",
    "    final_output = final_output[:, -1, :]\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(\n",
    "        final_output,\n",
    "        F.softmax(model.prediction_head(transformer_input), dim=-1)[:, -1, :],\n",
    "    )\n",
    "\n",
    "    return final_output\n",
    "\n",
    "\n",
    "# Lets make unit test for ClassiofierEncoder\n",
    "class ClassifierEncoderTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 3\n",
    "        self.d_k = 3\n",
    "        self.d_v = 3\n",
    "        self.t = 4\n",
    "        self.d_model = 3\n",
    "        self.h = 2\n",
    "        self.t_blocks = 2\n",
    "        self.vocab_size = 6\n",
    "\n",
    "        self.model = ClassifierEncoder(\n",
    "            t=self.t,\n",
    "            d_K=self.d_k,\n",
    "            d_V=self.d_v,\n",
    "            d_model=self.d_model,\n",
    "            h=self.h,\n",
    "            vocab_size=self.vocab_size,\n",
    "            n_transformers=self.t_blocks,\n",
    "            n_classes=N_CLASSES,\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def test_forward(self):\n",
    "        # Create a random input\n",
    "        x = getTestX(self.batch_size, self.t, self.vocab_size)\n",
    "\n",
    "        # define the weights of the model\n",
    "        with torch.no_grad():\n",
    "            # 1. define weights for the embedding layer\n",
    "            self.model.embedding.embedding = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        [0.1, 0.05, 0.02],\n",
    "                        [0.03, 0.07, 0.01],\n",
    "                        [0.04, 0.02, 0.08],\n",
    "                        [0.06, 0.03, 0.09],\n",
    "                        [0.02, 0.06, 0.04],\n",
    "                        [0.05, 0.01, 0.07],\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            # assert embedding has shape (vocab_size, d_model)\n",
    "            self.assertEqual(\n",
    "                self.model.embedding.embedding.shape,\n",
    "                torch.Size((self.vocab_size, self.d_model)),\n",
    "            )\n",
    "\n",
    "            # 2. define weights for the transformer blocks\n",
    "\n",
    "            # 2.1. define weights for the multihead attention layers\n",
    "            for t_idx in range(self.t_blocks):\n",
    "                for a_idx in range(self.h):\n",
    "                    self.model.transformer_blocks[t_idx].mha.attentions[\n",
    "                        a_idx\n",
    "                    ].W_Q = nn.Parameter(\n",
    "                        torch.tensor(\n",
    "                            [[0.1, 0.05, 0.02], [0.03, 0.07, 0.01], [0.04, 0.02, 0.08]]\n",
    "                        )\n",
    "                    )\n",
    "                    self.model.transformer_blocks[t_idx].mha.attentions[\n",
    "                        a_idx\n",
    "                    ].W_K = nn.Parameter(\n",
    "                        torch.tensor(\n",
    "                            [[0.06, 0.03, 0.09], [0.02, 0.08, 0.01], [0.07, 0.01, 0.04]]\n",
    "                        )\n",
    "                    )\n",
    "                    self.model.transformer_blocks[t_idx].mha.attentions[\n",
    "                        a_idx\n",
    "                    ].W_V = nn.Parameter(\n",
    "                        torch.tensor(\n",
    "                            [[0.05, 0.02, 0.1], [0.08, 0.01, 0.03], [0.01, 0.04, 0.07]]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # 2.2. define weights for the ANN layers\n",
    "                self.model.transformer_blocks[t_idx].ann[0].weight = nn.Parameter(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            [0.1, 0.05, 0.02],\n",
    "                            [0.03, 0.07, 0.01],\n",
    "                            [0.04, 0.02, 0.08],\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "                self.model.transformer_blocks[t_idx].ann[0].bias = nn.Parameter(\n",
    "                    torch.tensor([0.1, 0.05, 0.02])\n",
    "                )\n",
    "                self.model.transformer_blocks[t_idx].ann[3].weight = nn.Parameter(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            [0.1, 0.05, 0.02],\n",
    "                            [0.03, 0.07, 0.01],\n",
    "                            [0.04, 0.02, 0.08],\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "                self.model.transformer_blocks[t_idx].ann[3].bias = nn.Parameter(\n",
    "                    torch.tensor([0.1, 0.05, 0.02])\n",
    "                )\n",
    "\n",
    "                # 2.3. define weights for the W_O\n",
    "                # W_O must have shape (h * d_V, d_model)\n",
    "                self.model.transformer_blocks[t_idx].mha.W_O = nn.Parameter(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            [0.1, 0.05, 0.02],\n",
    "                            [0.03, 0.07, 0.01],\n",
    "                            [0.04, 0.02, 0.08],\n",
    "                            [0.06, 0.03, 0.09],\n",
    "                            [0.02, 0.08, 0.01],\n",
    "                            [0.07, 0.01, 0.04],\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # assert W_O has shape (h * d_V, d_model)\n",
    "                self.assertEqual(\n",
    "                    self.model.transformer_blocks[t_idx].mha.W_O.shape,\n",
    "                    torch.Size((self.h * self.d_v, self.d_model)),\n",
    "                )\n",
    "\n",
    "            # Now that wights are defined, let's in fact unittest the forward pass of the ClassifierEncoder\n",
    "            self.model = self.model\n",
    "            self.model.eval()\n",
    "            out = self.model(x)\n",
    "\n",
    "            # 1. test the shape of the output\n",
    "            self.assertEqual(out.shape, torch.Size((self.batch_size, N_CLASSES)))\n",
    "\n",
    "            # 2. test the values of the output\n",
    "            expected_output = calculate_expected_output(x, self.model)\n",
    "\n",
    "            self.assertTrue(torch.allclose(out, expected_output, atol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".........\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 0.185s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7efc76650940>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will run the test in the notebook\n",
    "RUN_UNIT_TESTS and unittest.main(argv=[\"\"], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
