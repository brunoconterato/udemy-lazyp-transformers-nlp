{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters Here\n",
    "batch_size = 8\n",
    "T = 10\n",
    "d_K = 2\n",
    "d_V = 4\n",
    "d_model = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "# Test softmax x axis:\n",
    "matrix = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "softmaxed_matrix = F.softmax(matrix, dim=1)\n",
    "\n",
    "print(softmaxed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(x: torch.Tensor, n: int):\n",
    "    # make shape (n, 1, 1, ...) --> quantity of 1's must be len(x.shape)\n",
    "    # for example, if shape of x is (3, 4, 8), shapee must be (n, 1, 1, 1)\n",
    "    tuple_ones = tuple(\n",
    "        (torch.tensor(x.shape) / torch.tensor(x.shape)).numpy().astype(int)\n",
    "    )\n",
    "    # print((n, *tuple_ones))\n",
    "    return x.unsqueeze(0).repeat((n, *tuple_ones))\n",
    "\n",
    "\n",
    "def batched_matmul(x_batched, W):\n",
    "    # # Assuming x_batched.shape == (batch_size, T, d_model)\n",
    "    # # and W.shape == (d_model, d)\n",
    "    # batch_size, T, d_model = x_batched.shape\n",
    "    # d = W.shape[1]\n",
    "\n",
    "    # # Reshape x_batched to (batch_size * T, d_model)\n",
    "    # x_reshaped = x_batched.reshape(-1, d_model)\n",
    "\n",
    "    # # Perform matrix multiplication\n",
    "    # result = torch.matmul(x_reshaped, W)\n",
    "\n",
    "    # # Reshape the result back to (batch_size, T, d)\n",
    "    # result = result.reshape(batch_size, T, d)\n",
    "\n",
    "    # return result\n",
    "\n",
    "    # batch_size = x_batched.shape[0]\n",
    "    # W_repeated = W.unsqueeze(0).repeat((batch_size, 1, 1))\n",
    "    W_repeated = repeat(W, n=x_batched.shape[0])\n",
    "    return torch.bmm(x_batched, W_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, T: int, d_K, d_V, d_model: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # define a torch 2d tensor initialized normally\n",
    "        self.W_K = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_K)))\n",
    "        self.W_Q = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_K)))\n",
    "        self.W_V = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_V)))\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Shapes:\n",
    "        # W_K (d_model, d_K)\n",
    "        # x is a 3d tensor (batch x T x d_model)\n",
    "\n",
    "        # W_K.T ->  (1, d_k x d_model)\n",
    "        # x ->      (batch, T, d_model)\n",
    "        K = batched_matmul(x, self.W_K)\n",
    "        Q = batched_matmul(x, self.W_Q)\n",
    "        V = batched_matmul(x, self.W_V)\n",
    "\n",
    "        result = torch.bmm(Q, K.transpose(1, 2)) / (K.shape[-1] ** 0.5)\n",
    "        if self.mask:\n",
    "            result = batched_matmul(result, self.mask)\n",
    "        result = F.softmax(result, dim=-1)\n",
    "        result = torch.bmm(result, V)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 2])\n",
      "torch.Size([6, 4])\n",
      "torch.Size([8, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "att = Attention(T=T, d_K=d_K, d_V=d_V, d_model=d_model)\n",
    "x = torch.normal(mean=0, std=0.01, size=(batch_size, T, d_model))\n",
    "\n",
    "att_result = att.forward(x)\n",
    "assert att_result.shape == (batch_size, T, d_V)\n",
    "print(att.W_K.shape)\n",
    "print(att.W_Q.shape)\n",
    "print(att.W_V.shape)\n",
    "print(att_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, h: int, T=T, d_K=d_K, d_V=d_V, d_model=d_model, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.h = h\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [Attention(T=T, d_K=d_K, d_model=d_model, d_V=d_V) for _ in range(h)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        attention_results = nn.ModuleList([])\n",
    "        for attention in self.attentions:\n",
    "            attention_result = attention(x)\n",
    "            print(attention_result)\n",
    "            assert False\n",
    "            attention_results.append(attention_result)\n",
    "            \n",
    "        print(attention_results)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3849e-05, -2.0673e-06,  7.9926e-05,  6.6457e-05],\n",
      "         [ 1.3849e-05, -2.0673e-06,  7.9926e-05,  6.6457e-05],\n",
      "         [ 1.3849e-05, -2.0673e-06,  7.9926e-05,  6.6457e-05],\n",
      "         [ 1.3849e-05, -2.0673e-06,  7.9926e-05,  6.6457e-05],\n",
      "         [ 1.3849e-05, -2.0673e-06,  7.9926e-05,  6.6457e-05],\n",
      "         [ 1.3849e-05, -2.0673e-06,  7.9926e-05,  6.6457e-05],\n",
      "         [ 1.3849e-05, -2.0673e-06,  7.9926e-05,  6.6457e-05],\n",
      "         [ 1.3849e-05, -2.0673e-06,  7.9926e-05,  6.6457e-05],\n",
      "         [ 1.3849e-05, -2.0673e-06,  7.9926e-05,  6.6457e-05],\n",
      "         [ 1.3849e-05, -2.0673e-06,  7.9926e-05,  6.6457e-05]],\n",
      "\n",
      "        [[ 2.8928e-05, -4.5233e-05, -8.8701e-05, -6.0039e-06],\n",
      "         [ 2.8928e-05, -4.5233e-05, -8.8701e-05, -6.0039e-06],\n",
      "         [ 2.8928e-05, -4.5233e-05, -8.8701e-05, -6.0039e-06],\n",
      "         [ 2.8928e-05, -4.5233e-05, -8.8701e-05, -6.0039e-06],\n",
      "         [ 2.8928e-05, -4.5233e-05, -8.8701e-05, -6.0039e-06],\n",
      "         [ 2.8928e-05, -4.5233e-05, -8.8701e-05, -6.0039e-06],\n",
      "         [ 2.8928e-05, -4.5233e-05, -8.8701e-05, -6.0039e-06],\n",
      "         [ 2.8928e-05, -4.5233e-05, -8.8701e-05, -6.0039e-06],\n",
      "         [ 2.8928e-05, -4.5233e-05, -8.8701e-05, -6.0039e-06],\n",
      "         [ 2.8928e-05, -4.5233e-05, -8.8701e-05, -6.0039e-06]],\n",
      "\n",
      "        [[ 7.8922e-05, -2.3708e-05,  3.0598e-05,  2.7457e-05],\n",
      "         [ 7.8922e-05, -2.3708e-05,  3.0598e-05,  2.7457e-05],\n",
      "         [ 7.8922e-05, -2.3708e-05,  3.0598e-05,  2.7457e-05],\n",
      "         [ 7.8922e-05, -2.3708e-05,  3.0598e-05,  2.7457e-05],\n",
      "         [ 7.8922e-05, -2.3708e-05,  3.0598e-05,  2.7457e-05],\n",
      "         [ 7.8922e-05, -2.3708e-05,  3.0598e-05,  2.7457e-05],\n",
      "         [ 7.8922e-05, -2.3708e-05,  3.0598e-05,  2.7457e-05],\n",
      "         [ 7.8922e-05, -2.3708e-05,  3.0598e-05,  2.7457e-05],\n",
      "         [ 7.8922e-05, -2.3708e-05,  3.0598e-05,  2.7457e-05],\n",
      "         [ 7.8922e-05, -2.3708e-05,  3.0598e-05,  2.7457e-05]],\n",
      "\n",
      "        [[ 1.0731e-04, -3.5313e-05,  5.8815e-05,  4.4494e-05],\n",
      "         [ 1.0731e-04, -3.5313e-05,  5.8815e-05,  4.4494e-05],\n",
      "         [ 1.0731e-04, -3.5313e-05,  5.8815e-05,  4.4494e-05],\n",
      "         [ 1.0731e-04, -3.5313e-05,  5.8815e-05,  4.4494e-05],\n",
      "         [ 1.0731e-04, -3.5313e-05,  5.8815e-05,  4.4494e-05],\n",
      "         [ 1.0731e-04, -3.5313e-05,  5.8815e-05,  4.4494e-05],\n",
      "         [ 1.0731e-04, -3.5313e-05,  5.8815e-05,  4.4494e-05],\n",
      "         [ 1.0731e-04, -3.5313e-05,  5.8815e-05,  4.4494e-05],\n",
      "         [ 1.0731e-04, -3.5313e-05,  5.8815e-05,  4.4494e-05],\n",
      "         [ 1.0731e-04, -3.5313e-05,  5.8815e-05,  4.4494e-05]],\n",
      "\n",
      "        [[ 2.5781e-04, -1.1037e-04,  7.8629e-05,  7.2109e-05],\n",
      "         [ 2.5781e-04, -1.1037e-04,  7.8629e-05,  7.2109e-05],\n",
      "         [ 2.5781e-04, -1.1037e-04,  7.8629e-05,  7.2109e-05],\n",
      "         [ 2.5781e-04, -1.1037e-04,  7.8629e-05,  7.2109e-05],\n",
      "         [ 2.5781e-04, -1.1037e-04,  7.8629e-05,  7.2109e-05],\n",
      "         [ 2.5781e-04, -1.1037e-04,  7.8629e-05,  7.2109e-05],\n",
      "         [ 2.5781e-04, -1.1037e-04,  7.8629e-05,  7.2109e-05],\n",
      "         [ 2.5781e-04, -1.1037e-04,  7.8629e-05,  7.2109e-05],\n",
      "         [ 2.5781e-04, -1.1037e-04,  7.8629e-05,  7.2109e-05],\n",
      "         [ 2.5781e-04, -1.1037e-04,  7.8629e-05,  7.2109e-05]],\n",
      "\n",
      "        [[ 6.9616e-05, -6.3533e-05, -1.5122e-05,  1.3869e-05],\n",
      "         [ 6.9616e-05, -6.3533e-05, -1.5122e-05,  1.3869e-05],\n",
      "         [ 6.9616e-05, -6.3533e-05, -1.5122e-05,  1.3869e-05],\n",
      "         [ 6.9616e-05, -6.3533e-05, -1.5122e-05,  1.3869e-05],\n",
      "         [ 6.9616e-05, -6.3533e-05, -1.5122e-05,  1.3869e-05],\n",
      "         [ 6.9616e-05, -6.3533e-05, -1.5122e-05,  1.3869e-05],\n",
      "         [ 6.9616e-05, -6.3533e-05, -1.5122e-05,  1.3869e-05],\n",
      "         [ 6.9616e-05, -6.3533e-05, -1.5122e-05,  1.3869e-05],\n",
      "         [ 6.9616e-05, -6.3533e-05, -1.5122e-05,  1.3869e-05],\n",
      "         [ 6.9616e-05, -6.3533e-05, -1.5122e-05,  1.3869e-05]],\n",
      "\n",
      "        [[-2.2284e-06,  6.2701e-07, -5.6814e-05,  5.3418e-05],\n",
      "         [-2.2283e-06,  6.2701e-07, -5.6814e-05,  5.3418e-05],\n",
      "         [-2.2283e-06,  6.2701e-07, -5.6814e-05,  5.3418e-05],\n",
      "         [-2.2283e-06,  6.2701e-07, -5.6814e-05,  5.3418e-05],\n",
      "         [-2.2284e-06,  6.2701e-07, -5.6814e-05,  5.3418e-05],\n",
      "         [-2.2283e-06,  6.2701e-07, -5.6814e-05,  5.3418e-05],\n",
      "         [-2.2283e-06,  6.2701e-07, -5.6814e-05,  5.3418e-05],\n",
      "         [-2.2283e-06,  6.2701e-07, -5.6814e-05,  5.3418e-05],\n",
      "         [-2.2283e-06,  6.2701e-07, -5.6814e-05,  5.3418e-05],\n",
      "         [-2.2283e-06,  6.2701e-07, -5.6814e-05,  5.3418e-05]],\n",
      "\n",
      "        [[-2.0458e-05,  1.0742e-05, -1.3133e-05,  8.7137e-05],\n",
      "         [-2.0458e-05,  1.0742e-05, -1.3133e-05,  8.7137e-05],\n",
      "         [-2.0458e-05,  1.0742e-05, -1.3133e-05,  8.7137e-05],\n",
      "         [-2.0458e-05,  1.0742e-05, -1.3133e-05,  8.7137e-05],\n",
      "         [-2.0458e-05,  1.0742e-05, -1.3133e-05,  8.7137e-05],\n",
      "         [-2.0458e-05,  1.0742e-05, -1.3133e-05,  8.7137e-05],\n",
      "         [-2.0458e-05,  1.0742e-05, -1.3133e-05,  8.7137e-05],\n",
      "         [-2.0458e-05,  1.0742e-05, -1.3133e-05,  8.7137e-05],\n",
      "         [-2.0458e-05,  1.0742e-05, -1.3133e-05,  8.7137e-05],\n",
      "         [-2.0458e-05,  1.0742e-05, -1.3133e-05,  8.7137e-05]]],\n",
      "       grad_fn=<BmmBackward0>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10762/2335922426.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_K\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_K\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_V\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/gpt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10762/2378997586.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mattention_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mattention_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(8, T=T, d_K=d_K, d_V=d_V, d_model=d_model)\n",
    "mha(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
