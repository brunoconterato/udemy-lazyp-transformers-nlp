{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Let's build the transformer's encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0. Init\n",
    "\n",
    "* Initialize the notebook environment.\n",
    "* Hyperparameters definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters Here\n",
    "BATCH_SIZE = 128\n",
    "T = 80  # sentence length\n",
    "D_K = 16\n",
    "D_V = 16\n",
    "D_MODEL = 128\n",
    "H = 8\n",
    "VOCAB_SIZE = 10\n",
    "N_TRANSFORMERS_BLOCKS_ENCODER = 6\n",
    "N_CLASSES = 2  # classes of classifier\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "RUN_UNIT_TESTS = True\n",
    "TRAIN_MODEL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "# Test softmax x axis:\n",
    "matrix = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "softmaxed_matrix = F.softmax(matrix, dim=1)\n",
    "\n",
    "print(softmaxed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(x: torch.Tensor, n: int):\n",
    "    # make shape (n, 1, 1, ...) --> quantity of 1's must be len(x.shape)\n",
    "    # for example, if shape of x is (3, 4, 8), shapee must be (n, 1, 1, 1)\n",
    "    tuple_ones = tuple(\n",
    "        (torch.tensor(x.shape) / torch.tensor(x.shape)).numpy().astype(int)\n",
    "    )\n",
    "    return x.unsqueeze(0).repeat((n, *tuple_ones))\n",
    "\n",
    "\n",
    "def batched_matmul(tensor_3d, tensor_2d):\n",
    "    W_repeated = repeat(tensor_2d, n=tensor_3d.shape[0])\n",
    "\n",
    "    return torch.bmm(tensor_3d, W_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Convention from: https://www.udemy.com/course/data-science-transformers-nlp/learn/lecture/32255056#overview\n",
    "    In our convention, K, Q and V are learneable, different from the \"Attention is all you need\" paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_K, d_V, d_model: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Define trainable parameters with requires_grad=True\n",
    "        # torch 2d tensor initialized normally\n",
    "        self.W_K = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_K)))\n",
    "        self.W_Q = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_K)))\n",
    "        self.W_V = nn.Parameter(torch.normal(mean=0, std=0.01, size=(d_model, d_V)))\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the Attention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch, T, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch, T, d_V)\n",
    "            torch.Tensor: Attention weights of shape (batch, T, T)\n",
    "        \"\"\"\n",
    "        # Shapes:\n",
    "        # x is a 3d tensor (batch, T, d_model)\n",
    "        # W_K (d_model, d_K)\n",
    "\n",
    "        # (batch, T, d_model) x (d_model, d_K) -> (batch, T, d_K)\n",
    "        K = batched_matmul(x, self.W_K)\n",
    "        Q = batched_matmul(x, self.W_Q)\n",
    "        V = batched_matmul(x, self.W_V)\n",
    "\n",
    "        # (batch, T, d_K) x (batch, d_k, T) -> (batch, T, T)\n",
    "        result = torch.bmm(Q, K.transpose(1, 2)) / (K.shape[-1] ** 0.5)\n",
    "        if self.mask:\n",
    "            result = torch.bmm(result, self.mask)\n",
    "        attention_weights = F.softmax(result, dim=-1)\n",
    "        # (batch, T, T) x (batch, T, d_V) -> (batch, T, d_V)\n",
    "        result = torch.bmm(attention_weights, V)\n",
    "        return result, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16])\n",
      "torch.Size([128, 16])\n",
      "torch.Size([128, 16])\n",
      "torch.Size([128, 80, 16])\n"
     ]
    }
   ],
   "source": [
    "att = Attention(d_K=D_K, d_V=D_V, d_model=D_MODEL)\n",
    "x = torch.normal(mean=0, std=0.01, size=(BATCH_SIZE, T, D_MODEL))\n",
    "\n",
    "att_result, att_weights = att.forward(x)\n",
    "assert att_result.shape == (BATCH_SIZE, T, D_V)\n",
    "print(att.W_K.shape)\n",
    "print(att.W_Q.shape)\n",
    "print(att.W_V.shape)\n",
    "print(att_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, d_K, d_V):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [Attention(d_K=d_K, d_V=d_V, d_model=d_model) for _ in range(h)]\n",
    "        )\n",
    "        self.W_O = nn.Parameter(torch.normal(mean=0, std=0.01, size=(h * d_V, d_model)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"Forward pass of the MultiHeadAttention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch, T, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch, T, d_model)\n",
    "            torch.Tensor: Attention weights of shape (batch, h, T, T)\n",
    "        \"\"\"\n",
    "        attention_results = [attention(x)[0] for attention in self.attentions]\n",
    "        attention_weights = torch.stack(\n",
    "            [attention(x)[1] for attention in self.attentions]\n",
    "        )\n",
    "        concatenated = torch.cat(attention_results, dim=-1)\n",
    "        return batched_matmul(concatenated, self.W_O), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(h=H, d_K=D_K, d_V=D_V, d_model=D_MODEL)\n",
    "out, att_weights = mha(x)\n",
    "assert out.shape == (BATCH_SIZE, T, D_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. The transformer block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (1): Dropout(p=0.1, inplace=False)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (4): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_K=D_K, d_V=D_V, d_model=D_MODEL, h=H, dropout=0.1, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mha = MultiHeadAttention(h, d_K=d_K, d_V=d_V, d_model=d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.layer_norm(x + self.mha(x)[0])\n",
    "        x = self.layer_norm(x + self.ann(x)[0])\n",
    "        return x\n",
    "\n",
    "\n",
    "transformerBlock = TransformerBlock()\n",
    "transformerBlock(x).shape\n",
    "transformerBlock.ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. The positional encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PositionalEncoding(t: int, d_model) -> torch.Tensor:\n",
    "    encodings = torch.zeros(size=(t, d_model), requires_grad=False)\n",
    "    counter = 0\n",
    "    for pos in range(t):\n",
    "        for i in range((d_model // 2) + 1):\n",
    "            if 2 * i < d_model:\n",
    "                counter += 1\n",
    "                encodings[pos, 2 * i] = torch.sin(\n",
    "                    pos / torch.tensor(10000).pow(2 * i / d_model)\n",
    "                )\n",
    "            if 2 * i + 1 < d_model:\n",
    "                counter += 1\n",
    "                encodings[pos, 2 * i + 1] = torch.cos(\n",
    "                    pos / torch.tensor(10000).pow(2 * i / d_model)\n",
    "                )\n",
    "    assert counter == t * d_model\n",
    "    return encodings\n",
    "\n",
    "\n",
    "PositionalEncoding(T, D_MODEL).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15108/2721896283.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  torch.range(0, 10).reshape(-1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.range(0, 10).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. The embedding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding tensor([[ 0.0558,  0.0400],\n",
      "        [-0.0617, -0.0197],\n",
      "        [ 0.0988, -0.1107]])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2, 2])\n",
      "tensor([[[ 0.0988, -0.1107],\n",
      "         [-0.0617, -0.0197]],\n",
      "\n",
      "        [[-0.0617, -0.0197],\n",
      "         [ 0.0558,  0.0400]],\n",
      "\n",
      "        [[ 0.0558,  0.0400],\n",
      "         [ 0.0988, -0.1107]]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: make this work in batches\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, padding_idx=0):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Parameter(\n",
    "            torch.normal(\n",
    "                mean=0.0, std=0.1, size=(vocab_size, d_model), requires_grad=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass Embedding layer\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input Tensor of shape (batch_size, T)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output Tensor of shape (batch_size, T, d_model)\n",
    "        \"\"\"\n",
    "        return self.embedding[x.long()]\n",
    "\n",
    "\n",
    "emb = Embedding(vocab_size=3, d_model=2)\n",
    "# emb = nn.Embedding(num_embeddings=3, embedding_dim=2)\n",
    "for name, param in emb.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "input_emb = torch.FloatTensor([[2, 1], [1, 0], [0, 2]])\n",
    "\n",
    "print(input_emb.shape)\n",
    "\n",
    "out_emb = emb(input_emb.long())\n",
    "print(out_emb.shape)\n",
    "print(out_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. The Classification Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make this class Encoder.\n",
    "# Then, make the ClassifierEncoder class be this class + a classifier head\n",
    "class ClassifierEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        t=T,\n",
    "        d_K=D_K,\n",
    "        d_V=D_V,\n",
    "        d_model=D_MODEL,\n",
    "        h=H,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        n_transformers=N_TRANSFORMERS_BLOCKS_ENCODER,\n",
    "        n_classes=N_CLASSES,\n",
    "        dropout=0.1,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ClassifierEncoder, self).__init__()\n",
    "        self.d_K = d_K\n",
    "        self.d_V = d_V\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.position_encoding: torch.Tensor = PositionalEncoding(t, d_model).to(device)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "        for _ in range(n_transformers):\n",
    "            self.transformer_blocks.append(\n",
    "                TransformerBlock(d_K=d_K, d_V=d_V, d_model=d_model, h=h)\n",
    "            )\n",
    "\n",
    "        self.prediction_head = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batchedPositionalEncoding = self.position_encoding.repeat(x.shape[0], 1, 1)\n",
    "        x = self.embedding(x.int()).float()\n",
    "        x = x + batchedPositionalEncoding\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.prediction_head(x)\n",
    "\n",
    "        x = F.softmax(x, dim=-1)\n",
    "\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2,  ..., 7, 8, 9],\n",
       "        [9, 0, 1,  ..., 6, 7, 8],\n",
       "        [8, 9, 0,  ..., 5, 6, 7],\n",
       "        ...,\n",
       "        [5, 6, 7,  ..., 2, 3, 4],\n",
       "        [4, 5, 6,  ..., 1, 2, 3],\n",
       "        [3, 4, 5,  ..., 0, 1, 2]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTestX(batch_size=BATCH_SIZE, t=T, vocab_size=VOCAB_SIZE):\n",
    "    x = torch.arange(start=0, end=batch_size * t).reshape(batch_size, t) % vocab_size\n",
    "\n",
    "    # each row (wor of index i) of x must be translocated by i positions\n",
    "    for i in range(batch_size):\n",
    "        x[i] = torch.roll(x[i], i)\n",
    "    return x\n",
    "\n",
    "\n",
    "getTestX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------\n",
      "x.shape: torch.Size([3, 4])\n",
      "x: tensor([[0, 1, 2, 3],\n",
      "        [1, 4, 5, 0],\n",
      "        [4, 5, 2, 3]])\n",
      "\n",
      "----------------------\n",
      "out.shape:  torch.Size([3, 2])\n",
      "out tensor([[0.5785, 0.4215],\n",
      "        [0.5676, 0.4324],\n",
      "        [0.5785, 0.4215]])\n",
      "\n",
      "----------------------\n",
      "expected_output.shape:  torch.Size([3, 2])\n",
      "expected_output tensor([[0.5785, 0.4215],\n",
      "        [0.5675, 0.4325],\n",
      "        [0.5785, 0.4215]])\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    matmul_qk = torch.matmul(Q, K.transpose(-2, -1))  # Scaled QK^T\n",
    "    d_k = Q.size(-1)\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(d_k)\n",
    "    attention_weights = F.softmax(\n",
    "        scaled_attention_logits, dim=-1\n",
    "    )  # Softmax over last axis (seq_len_k)\n",
    "    output = torch.matmul(attention_weights, V)  # Softmax(QK^T)V\n",
    "    return output\n",
    "\n",
    "\n",
    "def calculate_expected_output(x, model):\n",
    "    \"\"\"This function must calculate the expected output of the model, given the input x.\n",
    "    This function must not forward pass the model, but rather calculate the expected output\n",
    "    The expected output is function of the weights of the model, which are defined in the test_forward method\n",
    "    of the ClassifierEncoderTest class.\n",
    "    This function must follow the \"Attention is all you need\" paper.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of shape (batch, T)\n",
    "        model (_type_): _description_\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    t = x.shape[1]\n",
    "\n",
    "    embedded_x = model.embedding(x)\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(embedded_x, model.embedding(x))\n",
    "\n",
    "    def positional_encoding(max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        div_term = div_term.unsqueeze(0)\n",
    "\n",
    "        for i in range(max_len):\n",
    "            for j in range(0, d_model, 2):\n",
    "                pe[i, j] = torch.sin(position[i] * div_term[0, j // 2])\n",
    "                if j + 1 < d_model:\n",
    "                    pe[i, j + 1] = torch.cos(position[i] * div_term[0, j // 2])\n",
    "\n",
    "        return pe.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    position_encoding = positional_encoding(t, model.d_model)\n",
    "    assert position_encoding.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(position_encoding, model.position_encoding)\n",
    "\n",
    "    transformer_input = embedded_x + position_encoding\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(\n",
    "        transformer_input, model.embedding(x) + model.position_encoding\n",
    "    )\n",
    "\n",
    "    # Transformer Blocks\n",
    "    for block in model.transformer_blocks:\n",
    "        # Multi-Head Attention\n",
    "        mha_output = torch.zeros_like(transformer_input)\n",
    "        assert mha_output.shape == (batch_size, t, model.d_model)\n",
    "        head_outputs = torch.zeros((batch_size, t, model.d_V * model.h))\n",
    "        for head_idx, head in enumerate(block.mha.attentions):\n",
    "            Q = batched_matmul(transformer_input, head.W_Q)\n",
    "            K = batched_matmul(transformer_input, head.W_K)\n",
    "            V = batched_matmul(transformer_input, head.W_V)\n",
    "            head_output = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "            # health checking forward: assert model forward equals simulated forward\n",
    "            assert torch.allclose(head_output, head.forward(transformer_input)[0])\n",
    "\n",
    "            # Assert the shape of the output of the head,\n",
    "            # bases on the \"Attention is all you need\" paper\n",
    "            assert head_output.shape == (batch_size, t, model.d_V)\n",
    "\n",
    "            head_outputs[\n",
    "                :, :, model.d_V * head_idx : model.d_V * (head_idx + 1)\n",
    "            ] = head_output\n",
    "\n",
    "        mha_output += batched_matmul(head_outputs, block.mha.W_O)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(mha_output, block.mha(transformer_input)[0])\n",
    "\n",
    "        # Layer Normalization\n",
    "        normed_mha_output = block.layer_norm(\n",
    "            mha_output + transformer_input\n",
    "        )  # Assuming residual connection\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(\n",
    "            normed_mha_output,\n",
    "            block.layer_norm(block.mha(transformer_input)[0] + transformer_input),\n",
    "        )\n",
    "\n",
    "        # Do it now...\n",
    "        # 1. Linear\n",
    "        ann_output = (\n",
    "            torch.matmul(normed_mha_output, block.ann[0].weight.T) + block.ann[0].bias\n",
    "        )\n",
    "        assert ann_output.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(ann_output, block.ann[0](normed_mha_output))\n",
    "\n",
    "        # 2. Dropout\n",
    "        # ann_output = block.ann[1](ann_output)\n",
    "        # assert ann_output.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # 3. ReLU\n",
    "        ann_output_relu = F.relu(ann_output)\n",
    "        assert ann_output_relu.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(ann_output_relu, block.ann[2](ann_output))\n",
    "\n",
    "        # 4. Linear\n",
    "        ann_output_linear_2 = (\n",
    "            torch.matmul(ann_output_relu, block.ann[3].weight.T) + block.ann[3].bias\n",
    "        )\n",
    "        assert ann_output_linear_2.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(ann_output_linear_2, block.ann[3](ann_output_relu))\n",
    "\n",
    "        # 5. Dropout\n",
    "        # ann_output_linear_2 = block.ann[4](ann_output_linear_2)\n",
    "        # assert ann_output_linear_2.shape == (batch_size, t, model.d_model)\n",
    "\n",
    "        # Layer Normalization\n",
    "        normed_ann_output_linear_2 = block.layer_norm(\n",
    "            ann_output_linear_2 + normed_mha_output\n",
    "        )\n",
    "\n",
    "        # Health checking forward: assert model forward equals simulated forward\n",
    "        assert torch.allclose(\n",
    "            normed_ann_output_linear_2,\n",
    "            block.layer_norm(ann_output_linear_2 + normed_mha_output),\n",
    "        )\n",
    "\n",
    "        # Prepare for next block\n",
    "        transformer_input = normed_ann_output_linear_2\n",
    "        # print(\"\\n----------------------\")\n",
    "        # print(\"transformer output: \", transformer_input)\n",
    "\n",
    "    # Prediction Head\n",
    "\n",
    "    # 1. Linear\n",
    "    output_linear = (\n",
    "        torch.matmul(transformer_input, model.prediction_head.weight.T)\n",
    "        + model.prediction_head.bias\n",
    "    )\n",
    "    assert output_linear.shape == (batch_size, t, model.n_classes)\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(output_linear, model.prediction_head(transformer_input))\n",
    "\n",
    "    # 2. Dropout\n",
    "    # final_output = model.prediction_head[1](final_output)\n",
    "    # assert final_output.shape == (batch_size, t, model.n_classes)\n",
    "\n",
    "    # 3. Softmax\n",
    "    final_output = nn.Softmax(dim=-1)(output_linear)\n",
    "\n",
    "    # return only the last value of the sequence\n",
    "    final_output = final_output[:, -1, :]\n",
    "\n",
    "    # Health checking forward: assert model forward equals simulated forward\n",
    "    assert torch.allclose(\n",
    "        final_output,\n",
    "        F.softmax(model.prediction_head(transformer_input), dim=-1)[:, -1, :],\n",
    "    )\n",
    "\n",
    "    return final_output\n",
    "\n",
    "\n",
    "# Lets make unit test for ClassiofierEncoder\n",
    "class ClassifierEncoderTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 3\n",
    "        self.d_k = 3\n",
    "        self.d_v = 3\n",
    "        self.t = 4\n",
    "        self.d_model = 3\n",
    "        self.h = 2\n",
    "        self.t_blocks = 2\n",
    "        self.vocab_size = 6\n",
    "\n",
    "        self.model = ClassifierEncoder(\n",
    "            t=self.t,\n",
    "            d_K=self.d_k,\n",
    "            d_V=self.d_v,\n",
    "            d_model=self.d_model,\n",
    "            h=self.h,\n",
    "            vocab_size=self.vocab_size,\n",
    "            n_transformers=self.t_blocks,\n",
    "            n_classes=N_CLASSES,\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # print parameters of the model\n",
    "        # print(\"\\n----------------------\")\n",
    "        # for name, param in self.model.named_parameters():\n",
    "        #     print(name, param.data)\n",
    "\n",
    "    # unittest the forward pass\n",
    "    def test_forward(self):\n",
    "        # Create a random input\n",
    "        x = getTestX(self.batch_size, self.t, self.vocab_size)\n",
    "        print(\"\\n----------------------\")\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        print(f\"x: {x}\")\n",
    "\n",
    "        # define the weights of the model\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 1. define weights for the embedding layer\n",
    "            self.model.embedding.embedding = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        [0.1, 0.05, 0.02],\n",
    "                        [0.03, 0.07, 0.01],\n",
    "                        [0.04, 0.02, 0.08],\n",
    "                        [0.06, 0.03, 0.09],\n",
    "                        [0.02, 0.06, 0.04],\n",
    "                        [0.05, 0.01, 0.07],\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            # assert embedding has shape (vocab_size, d_model)\n",
    "            self.assertEqual(\n",
    "                self.model.embedding.embedding.shape,\n",
    "                torch.Size((self.vocab_size, self.d_model)),\n",
    "            )\n",
    "\n",
    "            # 2. define weights for the transformer blocks\n",
    "\n",
    "            # 2.1. define weights for the multihead attention layers\n",
    "            for t_idx in range(self.t_blocks):\n",
    "                for a_idx in range(self.h):\n",
    "                    self.model.transformer_blocks[t_idx].mha.attentions[\n",
    "                        a_idx\n",
    "                    ].W_Q = nn.Parameter(\n",
    "                        torch.tensor(\n",
    "                            [[0.1, 0.05, 0.02], [0.03, 0.07, 0.01], [0.04, 0.02, 0.08]]\n",
    "                        )\n",
    "                    )\n",
    "                    self.model.transformer_blocks[t_idx].mha.attentions[\n",
    "                        a_idx\n",
    "                    ].W_K = nn.Parameter(\n",
    "                        torch.tensor(\n",
    "                            [[0.06, 0.03, 0.09], [0.02, 0.08, 0.01], [0.07, 0.01, 0.04]]\n",
    "                        )\n",
    "                    )\n",
    "                    self.model.transformer_blocks[t_idx].mha.attentions[\n",
    "                        a_idx\n",
    "                    ].W_V = nn.Parameter(\n",
    "                        torch.tensor(\n",
    "                            [[0.05, 0.02, 0.1], [0.08, 0.01, 0.03], [0.01, 0.04, 0.07]]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # 2.2. define weights for the ANN layers\n",
    "                # self.model.transformer_blocks[t_idx].ann[0] = nn.Linear(\n",
    "                #     self.d_model, self.d_model, bias=True\n",
    "                # )\n",
    "                self.model.transformer_blocks[t_idx].ann[0].weight = nn.Parameter(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            [0.1, 0.05, 0.02],\n",
    "                            [0.03, 0.07, 0.01],\n",
    "                            [0.04, 0.02, 0.08],\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "                self.model.transformer_blocks[t_idx].ann[0].bias = nn.Parameter(\n",
    "                    torch.tensor([0.1, 0.05, 0.02])\n",
    "                )\n",
    "                # self.model.transformer_blocks[t_idx].ann[3] = nn.Linear(\n",
    "                #     self.d_model, self.d_model, bias=True\n",
    "                # )\n",
    "                self.model.transformer_blocks[t_idx].ann[3].weight = nn.Parameter(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            [0.1, 0.05, 0.02],\n",
    "                            [0.03, 0.07, 0.01],\n",
    "                            [0.04, 0.02, 0.08],\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "                self.model.transformer_blocks[t_idx].ann[3].bias = nn.Parameter(\n",
    "                    torch.tensor([0.1, 0.05, 0.02])\n",
    "                )\n",
    "\n",
    "                # 2.3. define weights for the W_O\n",
    "                # W_O must have shape (h * d_V, d_model)\n",
    "                self.model.transformer_blocks[t_idx].mha.W_O = nn.Parameter(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            [0.1, 0.05, 0.02],\n",
    "                            [0.03, 0.07, 0.01],\n",
    "                            [0.04, 0.02, 0.08],\n",
    "                            [0.06, 0.03, 0.09],\n",
    "                            [0.02, 0.08, 0.01],\n",
    "                            [0.07, 0.01, 0.04],\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # assert W_O has shape (h * d_V, d_model)\n",
    "                self.assertEqual(\n",
    "                    self.model.transformer_blocks[t_idx].mha.W_O.shape,\n",
    "                    torch.Size((self.h * self.d_v, self.d_model)),\n",
    "                )\n",
    "\n",
    "            # print model parameters\n",
    "            # print(\"\\n----------------------\")\n",
    "            # print(\"model.parameters()\")\n",
    "            # for name, param in self.model.named_parameters():\n",
    "            #     print(name, param.data)\n",
    "\n",
    "            # Now that wights are defined, let's in fact unittest the forward pass of the ClassifierEncoder\n",
    "            self.model.eval()\n",
    "            out = self.model(x)\n",
    "            print(\"\\n----------------------\")\n",
    "            print(\"out.shape: \", out.shape)\n",
    "            print(\"out\", out)\n",
    "\n",
    "            # 1. test the shape of the output\n",
    "            self.assertEqual(out.shape, torch.Size((self.batch_size, N_CLASSES)))\n",
    "\n",
    "            # 2. test the values of the output\n",
    "            expected_output = calculate_expected_output(x, self.model)\n",
    "            print(\"\\n----------------------\")\n",
    "            print(\"expected_output.shape: \", expected_output.shape)\n",
    "            print(\"expected_output\", expected_output)\n",
    "\n",
    "            self.assertTrue(torch.allclose(out, expected_output, atol=1e-3))\n",
    "\n",
    "\n",
    "test = ClassifierEncoderTest()\n",
    "test.setUp()\n",
    "test.test_forward()\n",
    "# RUN_UNIT_TESTS and unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Let's try a classification problem using the CLassification Encoder\n",
    "* See the file: Fine-Tuning (Intermediate)/Fine-Tunning Sentiment Custom Dataset + Labels.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, lets make a dataset with cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(\"../Fine-Tuning (Intermediate)/AirlineTweets.csv\")\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_[[\"airline_sentiment\", \"text\"]].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {\n",
    "    \"positive\": 1,\n",
    "    \"negative\": 0,\n",
    "    \"neutral\": 2,\n",
    "}\n",
    "df[\"target\"] = df[\"airline_sentiment\"].map(target_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df[\"target\"] != 2]\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_filtered[['text', 'target']]\n",
    "# Not documented info: targets must have the column name label\n",
    "# sentence may have other names, but not label\n",
    "df2.columns = ['sentence', 'label']\n",
    "df2.to_csv(\"data.csv\", index=False)\n",
    "!head data.csv\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"csv\", data_files=\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "def splitTrainTestValidation(dataset: Dataset, valid_size=0.1, test_size=0.1):\n",
    "    len_valid = int(len(dataset) * valid_size)\n",
    "    len_test = int(len(dataset) * test_size)\n",
    "\n",
    "    splited: DatasetDict = dataset.train_test_split(\n",
    "        len_valid + len_test, shuffle=False, seed=42\n",
    "    )\n",
    "    splited[\"validation\"] = splited[\"test\"]\n",
    "    del splited[\"test\"]\n",
    "\n",
    "    splited_2 = splited[\"validation\"].train_test_split(len_test, shuffle=True, seed=42)\n",
    "    splited[\"validation\"] = splited_2[\"train\"]\n",
    "    splited[\"test\"] = splited_2[\"test\"]\n",
    "\n",
    "    return splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = raw_dataset['train'].train_test_split(test_size=.3, seed=42)\n",
    "split = splitTrainTestValidation(raw_dataset[\"train\"], valid_size=0.1, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=T\n",
    "    )\n",
    "\n",
    "\n",
    "# tokenizer(\"This is an example\", truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = split.map(tokenize_fn, batched=False)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little notation here:\n",
    "# token is an int from the tokenizer\n",
    "# idx is our index, to use in our embedding\n",
    "\n",
    "token2idx = {0: 0}\n",
    "idx2token = {}\n",
    "\n",
    "all_tokens = [\n",
    "    element\n",
    "    for list_ids in tokenized_datasets[\"train\"][\"input_ids\"]\n",
    "    for element in list_ids\n",
    "]\n",
    "all_tokens = list(set(all_tokens))\n",
    "\n",
    "token_index = 0\n",
    "for token in all_tokens:\n",
    "    if token not in token2idx:\n",
    "        token2idx[token] = token_index\n",
    "        idx2token[token_index] = token\n",
    "        token_index += 1\n",
    "\n",
    "\n",
    "def filterSplit(splited_dataset):\n",
    "    \"\"\"For valid and test datasets, get only those which all inpu_ids is in splited_dataset['train']\"\"\"\n",
    "\n",
    "    for split in [\"validation\", \"test\"]:\n",
    "        # Filter the splited_dataset[split] to only keep the ids which are in splited_dataset['train']\n",
    "        splited_dataset[split] = splited_dataset[split].filter(\n",
    "            lambda x: all(token in token2idx for token in x[\"input_ids\"])\n",
    "        )\n",
    "\n",
    "    return splited_dataset\n",
    "\n",
    "\n",
    "filtered_datasets = filterSplit(tokenized_datasets)\n",
    "filtered_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos construir nosso dicionÃ¡rio de tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeIndex(batch):\n",
    "    batch[\"input_idx\"] = [token2idx[single] for single in batch[\"input_ids\"]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "data = filtered_datasets.map(makeIndex, batched=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert all same length\n",
    "list(set([len(l_idx) for l_idx in data[\"train\"][\"input_idx\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torch.Tensor(data[\"train\"][\"input_idx\"]).type(torch.int32)\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "target_train = torch.Tensor(data[\"train\"][\"label\"])\n",
    "\n",
    "# make one_hot\n",
    "target_train = F.one_hot(target_train.to(torch.int64), num_classes=N_CLASSES).float()\n",
    "\n",
    "target_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid = torch.Tensor(data[\"validation\"][\"input_idx\"]).type(torch.int32)\n",
    "data_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_valid = torch.Tensor(data[\"validation\"][\"label\"])\n",
    "\n",
    "# make one_hot\n",
    "target_valid = F.one_hot(target_valid.to(torch.int64), num_classes=N_CLASSES).float()\n",
    "\n",
    "target_valid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = torch.Tensor(data[\"test\"][\"input_idx\"]).type(torch.int32)\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test = torch.Tensor(data[\"test\"][\"label\"])\n",
    "\n",
    "# make one_hot\n",
    "target_test = F.one_hot(target_test.to(torch.int64), num_classes=N_CLASSES).float()\n",
    "\n",
    "target_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's train our model! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "log_interval = 10\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader, criterion, verbose=False):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in eval_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            if type(criterion) == torch.nn.modules.loss.NLLLoss:\n",
    "                loss += criterion(output, target.argmax(dim=-1)).item()\n",
    "            else:\n",
    "                loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            target_ = target.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target_).sum()\n",
    "\n",
    "    # loss /= len(eval_loader.dataset)\n",
    "    verbose and print(\n",
    "        \"\\nAverage loss: {:.6f}, Accuracy: {}/{} ({:.2f}%)\\n\".format(\n",
    "            loss,\n",
    "            correct,\n",
    "            len(eval_loader.dataset),\n",
    "            correct / len(eval_loader.dataset) * 100,\n",
    "        )\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        if type(criterion) == torch.nn.modules.loss.NLLLoss:\n",
    "            loss = criterion(output, target.argmax(dim=-1))\n",
    "        else:\n",
    "            loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # valid_loss = evaluate(model, validation_loader, criterion)\n",
    "        # if valid_loss < best_loss:\n",
    "        #     best_loss = valid_loss\n",
    "        #     torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]\n",
    "\n",
    "\n",
    "model = ClassifierEncoder(vocab_size=len(token2idx))\n",
    "\n",
    "train_dataset = MyDataset(data_train, target_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "valid_dataset = MyDataset(data_valid, target_valid)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = MyDataset(data_train, target_train)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# TODO: get optimizer and criterion from literature\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "best_loss = np.inf\n",
    "losses = []\n",
    "if TRAIN_MODEL:\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train(model, train_loader, optimizer, criterion, epoch)\n",
    "        loss = evaluate(model, valid_loader, criterion, verbose=True)\n",
    "        losses.append(loss)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            torch.save(model.state_dict(), \"model.pth\")\n",
    "        print(f\"Epoch: {epoch}\\tloss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit tests for transformer and attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestTensorFunctions(unittest.TestCase):\n",
    "    def test_repeat(self):\n",
    "        x = torch.tensor([1, 2, 3])\n",
    "        n = 2\n",
    "        result = repeat(x, n)\n",
    "        expected = torch.tensor([[1, 2, 3], [1, 2, 3]])\n",
    "        self.assertTrue(torch.equal(result, expected))\n",
    "\n",
    "    def test_batched_matmul(self):\n",
    "        tensor_3d = torch.randn(10, 3, 4)\n",
    "        tensor_2d = torch.randn(4, 5)\n",
    "        result = batched_matmul(tensor_3d, tensor_2d)\n",
    "        expected = torch.bmm(tensor_3d, tensor_2d.unsqueeze(0).repeat((10, 1, 1)))\n",
    "        self.assertTrue(torch.allclose(result, expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class EmbeddingLayerTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Set up your embedding layer or load your model\n",
    "        self.vocab_size = 10\n",
    "        self.d_model = 8\n",
    "        self.batch_size = 2\n",
    "        self.t = 4\n",
    "        self.model = Embedding(self.vocab_size, self.d_model)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def test_embedding_layer_forward(self):\n",
    "        # Define input tensor (batch_size=2, sequence_length=4)\n",
    "        x = torch.tensor([[1, 3, 5, 7], [0, 2, 4, 6]])\n",
    "\n",
    "        # Manually set model weights for the embedding matrix\n",
    "        self.model.embedding.data = torch.tensor(\n",
    "            [\n",
    "                [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "                [0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6],\n",
    "                [1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4],\n",
    "                [2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2],\n",
    "                [3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0],\n",
    "                [4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8],\n",
    "                [4.9, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6],\n",
    "                [5.7, 5.8, 5.9, 6.0, 6.1, 6.2, 6.3, 6.4],\n",
    "                [6.5, 6.6, 6.7, 6.8, 6.9, 7.0, 7.1, 7.2],\n",
    "                [7.3, 7.4, 7.5, 7.6, 7.7, 7.8, 7.9, 8.0],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Calculate the expected output manually\n",
    "        expected_output = torch.tensor(\n",
    "            [\n",
    "                [\n",
    "                    [\n",
    "                        [0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6],\n",
    "                        [2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2],\n",
    "                        [4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8],\n",
    "                        [5.7, 5.8, 5.9, 6.0, 6.1, 6.2, 6.3, 6.4],\n",
    "                    ],\n",
    "                    [\n",
    "                        [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "                        [1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4],\n",
    "                        [3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0],\n",
    "                        [4.9, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6],\n",
    "                    ],\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Check if the shape of the expected output matches\n",
    "        self.assertEqual(expected_output.shape, expected_output.shape)\n",
    "\n",
    "        # Check if the values of the expected output tensor match\n",
    "        self.assertTrue(torch.allclose(expected_output, expected_output, atol=1e-6))\n",
    "\n",
    "    def test_embedding_layer_loss(self):\n",
    "        # Define input tensors and target tensors\n",
    "        input_tensor = torch.randint(\n",
    "            0, self.vocab_size, (self.batch_size, self.t)\n",
    "        ).long()\n",
    "        target_tensor = torch.randn(self.batch_size, self.t, self.d_model)\n",
    "\n",
    "        # Forward pass through the embedding layer\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.criterion(output, target_tensor)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param.grad != 0), \"No gradients in the parameters\"\n",
    "            )\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )\n",
    "\n",
    "\n",
    "# This will run the test in the notebook\n",
    "# RUN_UNIT_TESTS and unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAttention(unittest.TestCase):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        unittest (_type_): unit test for a conjuntion linear layer\n",
    "            plus the attention block\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class AttentionTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 2\n",
    "        self.d_k = 3\n",
    "        self.d_v = 3\n",
    "        self.T = 2\n",
    "        self.d_model = 4\n",
    "\n",
    "        # Set up your attention mechanism or load your model\n",
    "        self.model = Attention(self.d_k, self.d_v, self.d_model)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def test_attention_forward(self):\n",
    "        # Define input tensor (batch_size=2, sequence_length=2, d_model=4)\n",
    "        x = torch.tensor(\n",
    "            [\n",
    "                [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]],\n",
    "                [[9.0, 10.0, 11.0, 12.0], [13.0, 14.0, 15.0, 16.0]],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Manually set model weights for W_K, W_Q, and W_V\n",
    "        self.model.W_K.data = torch.tensor(\n",
    "            [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.model.W_Q.data = torch.tensor(\n",
    "            [[1.2, 1.1, 1.0], [0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.model.W_V.data = torch.tensor(\n",
    "            [[0.5, 0.6, 0.7], [0.8, 0.9, 1.0], [1.1, 1.2, 1.3], [1.4, 1.5, 1.6]],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "\n",
    "        # Expected output based on calculations from the attention mechanism\n",
    "        expected_output, _ = self.model(x)\n",
    "\n",
    "        # Calculate the expected output manually\n",
    "        # Calculate Q, K, and V using learned weights\n",
    "        Q = torch.matmul(x, self.model.W_Q)\n",
    "        K = torch.matmul(x, self.model.W_K)\n",
    "        V = torch.matmul(x, self.model.W_V)\n",
    "\n",
    "        # Calculate the attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(1, 2)) / (self.d_k**0.5)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Apply attention weights to V to get the output\n",
    "        expected_output_manual = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Check if the shape of the expected output matches\n",
    "        self.assertEqual(expected_output.shape, expected_output_manual.shape)\n",
    "\n",
    "        # Check if the values of the expected output tensor match\n",
    "        self.assertTrue(\n",
    "            torch.allclose(expected_output, expected_output_manual, atol=1e-6)\n",
    "        )\n",
    "\n",
    "    def test_attention_loss(self):\n",
    "        # Test the loss function associated with the attention mechanism\n",
    "        # Define input tensors and target tensors\n",
    "\n",
    "        input_tensor = torch.rand(\n",
    "            (self.batch_size, self.T, self.d_model), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # Forward pass through the attention mechanism\n",
    "        output, _ = self.model(input_tensor)\n",
    "\n",
    "        # Create dummy target tensor (replace with your actual target tensor)\n",
    "        target = torch.rand_like(output, requires_grad=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.criterion(output, target)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if gradients are computed\n",
    "        self.assertTrue(\n",
    "            input_tensor.grad is not None, \"No gradients in the input tensor\"\n",
    "        )\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )\n",
    "\n",
    "\n",
    "# This will run the test in the notebook\n",
    "# RUN_UNIT_TESTS and unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code by GPT4 to calculate the multihead attention output\n",
    "\n",
    "import torch\n",
    "\n",
    "# Define the parameters\n",
    "d_k = 3\n",
    "d_v = 3\n",
    "d_model = 4\n",
    "h = 2\n",
    "batch_size = 2\n",
    "t = 2\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = torch.tensor(\n",
    "    [\n",
    "        [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]],\n",
    "        [[9.0, 10.0, 11.0, 12.0], [13.0, 14.0, 15.0, 16.0]],\n",
    "    ],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Define the weights\n",
    "W_K = torch.tensor(\n",
    "    [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "W_Q = torch.tensor(\n",
    "    [[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 1.0], [1.1, 1.2, 1.3]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "W_V = torch.tensor(\n",
    "    [[0.3, 0.4, 0.5], [0.6, 0.7, 0.8], [0.9, 1.0, 1.1], [1.2, 1.3, 1.4]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "W_O = torch.tensor(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4],\n",
    "        [0.5, 0.6, 0.7, 0.8],\n",
    "        [0.9, 1.0, 1.1, 1.2],\n",
    "        [1.3, 1.4, 1.5, 1.6],\n",
    "        [1.7, 1.8, 1.9, 2.0],\n",
    "        [2.1, 2.2, 2.3, 2.4],\n",
    "    ],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Initialize tensors for K, Q, V for each head\n",
    "Ks, Qs, Vs = [], [], []\n",
    "\n",
    "# Compute K, Q, V for each head\n",
    "for _ in range(h):\n",
    "    K = torch.matmul(input_tensor, W_K)\n",
    "    Q = torch.matmul(input_tensor, W_Q)\n",
    "    V = torch.matmul(input_tensor, W_V)\n",
    "    Ks.append(K)\n",
    "    Qs.append(Q)\n",
    "    Vs.append(V)\n",
    "\n",
    "# Initialize a tensor for the concatenated results\n",
    "concatenated_results = torch.zeros(batch_size, t, h * d_v)\n",
    "\n",
    "# Calculate the attention and concatenate results for each head\n",
    "for a_idx in range(h):\n",
    "    # Scaled dot product of Q and K\n",
    "    attention_scores = torch.matmul(Qs[a_idx], Ks[a_idx].transpose(-2, -1)) / (\n",
    "        d_k**0.5\n",
    "    )\n",
    "    attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    # Weighted sum of V\n",
    "    head_output = torch.matmul(attention_weights, Vs[a_idx])\n",
    "\n",
    "    # Concatenate results\n",
    "    concatenated_results[:, :, a_idx * d_v : (a_idx + 1) * d_v] = head_output\n",
    "\n",
    "# Apply the final linear layer W_O\n",
    "expected_output = torch.matmul(concatenated_results, W_O)\n",
    "\n",
    "print(expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Initialize the multi-head attention model\n",
    "        self.model = MultiHeadAttention(self.h, self.d_model, self.d_k, self.d_v)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Manually setting the weights for the test\n",
    "        for head in self.model.attentions:\n",
    "            # These weights should be carefully chosen to ensure a predictable output\n",
    "            head.W_K = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]\n",
    "                )\n",
    "            )\n",
    "            head.W_Q = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 1.0], [1.1, 1.2, 1.3]]\n",
    "                )\n",
    "            )\n",
    "            head.W_V = nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [[0.3, 0.4, 0.5], [0.6, 0.7, 0.8], [0.9, 1.0, 1.1], [1.2, 1.3, 1.4]]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output linear layer weights\n",
    "        self.model.W_O = nn.Parameter(\n",
    "            torch.tensor(\n",
    "                [\n",
    "                    [0.1, 0.2, 0.3, 0.4],\n",
    "                    [0.5, 0.6, 0.7, 0.8],\n",
    "                    [0.9, 1.0, 1.1, 1.2],\n",
    "                    [1.3, 1.4, 1.5, 1.6],\n",
    "                    [1.7, 1.8, 1.9, 2.0],\n",
    "                    [2.1, 2.2, 2.3, 2.4],\n",
    "                ],\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Shape of W_O, according to the paper \"Attention Is All You Need\"\n",
    "        self.assertEqual(list(self.model.W_O.shape), [self.h * self.d_v, self.d_model])\n",
    "\n",
    "    def test_multi_head_attention_forward(self):\n",
    "        # Define a specific input tensor (batch_size, t, d_model)\n",
    "        input_tensor = torch.tensor(\n",
    "            [\n",
    "                [[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]],\n",
    "                [[9.0, 10.0, 11.0, 12.0], [13.0, 14.0, 15.0, 16.0]],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Expected output tensor based on the input and the model weights\n",
    "        # This tensor should be calculated manually based on the model's operations\n",
    "        expected_output = torch.Tensor(\n",
    "            [\n",
    "                [\n",
    "                    [159.9200, 174.0800, 188.2400, 202.4000],\n",
    "                    [159.9200, 174.0800, 188.2400, 202.4000],\n",
    "                ],\n",
    "                [\n",
    "                    [344.5600, 375.0400, 405.5200, 436.0000],\n",
    "                    [344.5600, 375.0400, 405.5200, 436.0000],\n",
    "                ],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Run the forward pass\n",
    "        output, _ = self.model(input_tensor)\n",
    "\n",
    "        # Check the shape of the output tensor\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.t, self.d_model))\n",
    "\n",
    "        # Check the values of the output tensor\n",
    "        self.assertTrue(torch.allclose(output, expected_output, atol=1e-4))\n",
    "\n",
    "    def test_multi_head_attention_loss(self):\n",
    "        # Test the loss function associated with the multi-head attention mechanism\n",
    "        # Define input tensors and target tensors\n",
    "\n",
    "        input_tensor = torch.rand(\n",
    "            (self.batch_size, self.t, self.d_model), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # Forward pass through the multi-head attention mechanism\n",
    "        output, _ = self.model(input_tensor)\n",
    "\n",
    "        # Create dummy target tensor (replace with your actual target tensor)\n",
    "        target = torch.rand_like(output, requires_grad=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.criterion(output, target)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if gradients are computed\n",
    "        self.assertTrue(\n",
    "            input_tensor.grad is not None, \"No gradients in the input tensor\"\n",
    "        )\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )\n",
    "\n",
    "\n",
    "# This will run the test in the notebook\n",
    "# RUN_UNIT_TESTS and unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_output(x, attentions, W_O):\n",
    "    batch_size, T, d_model = x.shape\n",
    "    h = len(attentions)\n",
    "    d_k, d_v = attentions[0].W_K.shape[1], attentions[0].W_V.shape[1]\n",
    "\n",
    "    # Initialize the tensor to hold the concatenated outputs from each head\n",
    "    concatenated_heads = torch.zeros((batch_size, T, h * d_v))\n",
    "\n",
    "    for i in range(h):\n",
    "        W_K, W_Q, W_V = attentions[i].W_K, attentions[i].W_Q, attentions[i].W_V\n",
    "\n",
    "        # Calculate K, Q, V matrices\n",
    "        K = x @ W_K\n",
    "        Q = x @ W_Q\n",
    "        V = x @ W_V\n",
    "\n",
    "        # Calculate attention scores and apply softmax (simplified here)\n",
    "        attention_scores = (Q @ K.transpose(-2, -1)) / (d_k**0.5)\n",
    "        # In a real scenario, apply softmax to attention_scores here\n",
    "\n",
    "        # Calculate the weighted sum of V\n",
    "        weighted_sum = attention_scores @ V\n",
    "\n",
    "        # Concatenate results from each head\n",
    "        concatenated_heads[:, :, i * d_v : (i + 1) * d_v] = weighted_sum\n",
    "\n",
    "    # Multiply with W_O\n",
    "    expected_output = concatenated_heads @ W_O\n",
    "    return expected_output\n",
    "\n",
    "\n",
    "class TransformerBlockTest(unittest.TestCase):\n",
    "    def calculate_attention_output(self, x, W_K, W_Q, W_V, d_k):\n",
    "        \"\"\"\n",
    "        Calculate the output of a single head attention.\n",
    "        \"\"\"\n",
    "        K = x @ W_K\n",
    "        Q = x @ W_Q\n",
    "        V = x @ W_V\n",
    "\n",
    "        attention_scores = (Q @ K.transpose(-2, -1)) / (d_k**0.5)\n",
    "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = attention_scores @ V\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "    def calculate_expected_output(\n",
    "        self, x, W_Ks, W_Qs, W_Vs, W_O, linear_weights, linear_biases\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculate the expected output of the TransformerBlock.\n",
    "        \"\"\"\n",
    "        batch_size, T, d_model = x.shape\n",
    "        h = len(W_Ks)\n",
    "        d_k, d_v = W_Ks[0].shape[1], W_Vs[0].shape[1]\n",
    "\n",
    "        # Step 1: Multi-head attention\n",
    "        concatenated_heads = torch.zeros((batch_size, T, h * d_v))\n",
    "        for i in range(h):\n",
    "            attention_output = self.calculate_attention_output(\n",
    "                x, W_Ks[i], W_Qs[i], W_Vs[i], d_k\n",
    "            )\n",
    "            concatenated_heads[:, :, i * d_v : (i + 1) * d_v] = attention_output\n",
    "\n",
    "        multihead_output = concatenated_heads @ W_O\n",
    "\n",
    "        # Step 2: Apply LayerNorm (simplified version)\n",
    "        norm_output = F.layer_norm(multihead_output, multihead_output.shape[1:])\n",
    "\n",
    "        # Step 3: ANN layers (assuming two linear layers with ReLU and Dropout in between)\n",
    "        ann_output = F.linear(norm_output, linear_weights[0], linear_biases[0])\n",
    "        ann_output = F.dropout(ann_output, p=0.1)\n",
    "        ann_output = F.relu(ann_output)\n",
    "        ann_output = F.linear(ann_output, linear_weights[1], linear_biases[1])\n",
    "\n",
    "        return ann_output\n",
    "\n",
    "    def setUp(self):\n",
    "        self.batch_size = 2\n",
    "        self.d_k = 3\n",
    "        self.d_v = 3\n",
    "        self.t = 4\n",
    "        self.d_model = 3\n",
    "        self.h = 2\n",
    "        # Set up your Transformer block model\n",
    "        self.model = TransformerBlock(\n",
    "            d_K=self.d_k, d_V=self.d_v, d_model=self.d_model, h=self.h\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "def test_transformer_block_forward(self):\n",
    "    # Define the input tensor\n",
    "    x = torch.tensor(\n",
    "        [\n",
    "            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]],\n",
    "            [\n",
    "                [13.0, 14.0, 15.0],\n",
    "                [16.0, 17.0, 18.0],\n",
    "                [19.0, 20.0, 21.0],\n",
    "                [22.0, 23.0, 24.0],\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Manually set and collect the weights for each Attention layer\n",
    "    W_Ks = []\n",
    "    W_Qs = []\n",
    "    W_Vs = []\n",
    "    for i, attention in enumerate(self.model.mha.attentions):\n",
    "        W_K = torch.tensor(\n",
    "            [\n",
    "                [0.1 * (i + 1), 0.2 * (i + 1), 0.3 * (i + 1)],\n",
    "                [0.4 * (i + 1), 0.5 * (i + 1), 0.6 * (i + 1)],\n",
    "                [0.7 * (i + 1), 0.8 * (i + 1), 0.9 * (i + 1)],\n",
    "            ]\n",
    "        )\n",
    "        W_Q = torch.tensor(\n",
    "            [\n",
    "                [0.9 * (i + 1), 0.8 * (i + 1), 0.7 * (i + 1)],\n",
    "                [0.6 * (i + 1), 0.5 * (i + 1), 0.4 * (i + 1)],\n",
    "                [0.3 * (i + 1), 0.2 * (i + 1), 0.1 * (i + 1)],\n",
    "            ]\n",
    "        )\n",
    "        W_V = torch.tensor(\n",
    "            [\n",
    "                [0.1 * (i + 1), 0.1 * (i + 1), 0.1 * (i + 1)],\n",
    "                [0.2 * (i + 1), 0.2 * (i + 1), 0.2 * (i + 1)],\n",
    "                [0.3 * (i + 1), 0.3 * (i + 1), 0.3 * (i + 1)],\n",
    "            ]\n",
    "        )\n",
    "        attention.W_K.data = W_K\n",
    "        attention.W_Q.data = W_Q\n",
    "        attention.W_V.data = W_V\n",
    "        W_Ks.append(W_K)\n",
    "        W_Qs.append(W_Q)\n",
    "        W_Vs.append(W_V)\n",
    "\n",
    "    # Set and collect weights for W_O in MultiHeadAttention\n",
    "    W_O = torch.tensor(\n",
    "        [\n",
    "            [0.1, 0.2, 0.3],\n",
    "            [0.4, 0.5, 0.6],\n",
    "            [0.7, 0.8, 0.9],\n",
    "            [0.9, 0.8, 0.7],\n",
    "            [0.6, 0.5, 0.4],\n",
    "            [0.3, 0.2, 0.1],\n",
    "        ]\n",
    "    )\n",
    "    self.model.mha.W_O.data = W_O\n",
    "\n",
    "    # Collect weights and biases for ANN layers (Assuming two linear layers)\n",
    "    linear_weights = [self.model.ann[0].weight, self.model.ann[3].weight]\n",
    "    linear_biases = [self.model.ann[0].bias, self.model.ann[3].bias]\n",
    "\n",
    "    # Calculate the expected output tensor\n",
    "    expected_output = self.calculate_expected_output(\n",
    "        x, W_Ks, W_Qs, W_Vs, W_O, linear_weights, linear_biases\n",
    "    )\n",
    "\n",
    "    # Forward pass\n",
    "    output, _ = self.model(x)\n",
    "\n",
    "    # Check the shape of the output tensor\n",
    "    self.assertEqual(output.shape, expected_output.shape)\n",
    "\n",
    "    # Check the values of the output tensor\n",
    "    self.assertTrue(torch.allclose(output, expected_output, atol=1e-6))\n",
    "\n",
    "    def test_transformer_block_loss(self):\n",
    "        # Test the loss function associated with the Transformer block\n",
    "        # Define input tensors and target tensors\n",
    "\n",
    "        input_tensor = torch.rand(\n",
    "            (self.batch_size, self.t, self.d_model), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # Forward pass through the Transformer block\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # Create dummy target tensor (replace with your actual target tensor)\n",
    "        target = torch.rand_like(output, requires_grad=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.criterion(output, target)\n",
    "\n",
    "        # Define an optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        # Save the parameters before the backward pass and parameter update\n",
    "        params_before = [param.clone() for param in self.model.parameters()]\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Check if gradients are computed\n",
    "        self.assertTrue(\n",
    "            input_tensor.grad is not None, \"No gradients in the input tensor\"\n",
    "        )\n",
    "\n",
    "        # Check if the parameters have been updated\n",
    "        for param, param_before in zip(self.model.parameters(), params_before):\n",
    "            self.assertTrue(\n",
    "                torch.any(param != param_before), \"Parameters have not been updated\"\n",
    "            )\n",
    "\n",
    "\n",
    "# This will run the test in the notebook\n",
    "# RUN_UNIT_TESTS and unittest.main(argv=[\"\"], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run the test in the notebook\n",
    "RUN_UNIT_TESTS and unittest.main(argv=[\"\"], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
